{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Immigration Pseudo-Labeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNqvXSDX4ta7Pfgwtzm1D5a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hlab-repo/purity-and-danger/blob/master/Immigration_Pseudo_Labeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGNecY0ujs2w"
      },
      "source": [
        "# Creating a Model for Immigration and \n",
        "\n",
        "---\n",
        "\n",
        "Outsider Language\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This notebook starts with a baseline system and then provides users the opportunity to attempt to improve performance with their own custom, complete system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BiHtT8qkT38"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMeyDaZAn70Q"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9fKFwGsP22I"
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import datasets\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YKxr_AzwQNh"
      },
      "source": [
        "We can start with the Common Crawl news corpus (January 2017 - December 2019). See here for details:\n",
        "\n",
        "https://huggingface.co/datasets/cc_news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH8YRQzskXCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed84c31-a625-45ac-8f35-80db2fb7f485"
      },
      "source": [
        "# this could take several minutes\n",
        "dataset = datasets.load_dataset('cc_news')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset cc_news (/root/.cache/huggingface/datasets/cc_news/plain_text/1.0.0/6cdde8d7fdaae3e50fb61b5d08d5387c2f0bbea1ee68755ef954af539a6a3a1b)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlx6xoDidiTa",
        "outputId": "8520fa72-908a-450b-a6f5-674ba889ac65"
      },
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['title', 'text', 'domain', 'date', 'description', 'url', 'image_url'],\n",
              "        num_rows: 708241\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEif50ojdmoa",
        "outputId": "10adf855-ec24-4280-e074-7c0256ca62c7"
      },
      "source": [
        "# look at the first 10 samples\n",
        "for i, s in enumerate(dataset['train']):\n",
        "    print(s)\n",
        "    if i >= 10:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'date': '2017-04-17 00:00:00', 'description': \"Officials unsealed court documents Monday (April 17) to reveal details surrounding the first searches of Prince's Paisley Park estate.\", 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/04/prince-young-and-sad.jpg?w=946', 'text': 'By Abby Hassler\\nOfficials unsealed court documents Monday (April 17) to reveal details surrounding the first searches of Prince’s Paisley Park estate following his untimely death.\\nRelated: Prince’s Ex-Wife Mayte Garcia Says Memoir is not a Tell-All\\nThe unsealed search warrants don’t confirm the source of the drug, fentanyl, that led to the 57-year-old singer’s accidental, self-administered overdose last April, according to The Star Tribune.\\nInvestigators found no prescriptions in Prince’s name, however, Dr. Michael Todd Schulenberg told detectives he had written a prescription for oxycodone, which is also an opioid, under the name of long-time Prince associate and drummer Kirk Johnson.\\nBetween April 21 and Sept. 19, 2016, Carver County authorities conducted investigations into Prince’s death with a total of 11 search warrants.', 'title': 'Prince Search Warrants Unsealed, Answer Few Questions', 'url': 'http://1041jackfm.cbslocal.com/2017/04/17/prince-search-warrants-unsealed-2/'}\n",
            "{'date': '2017-08-14 00:00:00', 'description': '\"The spirit of Green Day has always been about rising above oppression.\"', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/08/billie-joe-armstrong-theo-wargo-getty-images.jpg?w=946', 'text': 'By Abby Hassler\\nGreen Day’s Billie Joe Armstrong has always been outspoken about his political beliefs. Following the tragedy in Charlottesville, Virgina, over the weekend, Armstrong felt the need to speak out against the white supremacists who caused much of the violence.\\nRelated: Billie Joe Armstrong Wins #TBT with Childhood Studio Photo\\n“My heart feels heavy. I feel like what happened in Charlottesville goes beyond the point of anger,” Armstrong wrote on Facebook. “It makes me sad and desperate. shocked. I f—— hate racism more than anything.”\\n“The spirit of Green Day has always been about rising above oppression. and sticking up for what you believe in and singing it at the top of your lungs,” Armstrong continued. “We grew up fearing nuclear holocaust because of the cold war. those days are feeling way too relevant these days. these issues are our ugly past.. and now it’s coming to haunt us. always resist these doomsday politicians. and in the words of our punk forefathers .. Nazi punks f— off.”', 'title': 'Green Day’s Billie Joe Armstrong Rails Against White Nationalists', 'url': 'http://1041jackfm.cbslocal.com/2017/08/14/billie-joe-armstrong-white-nationalists/'}\n",
            "{'date': '2017-02-15 00:00:00', 'description': 'The concept, which has broken YouTube records and dominated social media, remains as simple and delightful as ever. Only this time, a rotating cast of celebrities will replace James Corden.', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/02/metallica-carpool.jpg?w=946', 'text': 'By Hayden Wright\\nA trailer has been released for the next sequence of Carpool Karaoke videos, and the lineup is stellar. The CBS late-night segment will stream on Apple Music in a similar format, but with new guests and even crazier moments behind the wheel. The concept, which has broken YouTube records and dominated social media, remains as simple and delightful as ever. Only this time, a rotating cast of celebrities will replace James Corden.\\nRelated: Adele’s ‘Carpool Karaoke’ is the Most Popular Viral Video of 2016\\nHere are the moments we’re most excited for:\\nMetallica making their headbanging mark on Rihanna’s “Diamonds” with Billy Eichner.\\nJohn Legend duetting on with Alicia Keys on her breakout hit, “Falling.”\\nAriana Grande belting the Little Shop of Horrors soundtrack with Seth MacFarlane.\\nJames Corden’s return with Will Smith — they rap the Fresh Prince theme!\\nChelsea Handler slinging whiskey and singing Bon Jovi’s “Living on a Prayer” with Blake Shelton.\\nCorden also presides over an epic, R. Kelly-inspired key change for the Carpool Karaoke franchise — there’s a helicopter.\\nA new installment will drop on Apple Music each week and the premiere is “coming soon.”\\nWatch the preview here:', 'title': 'Upcoming ‘Carpool Karaoke’ Series Looks Epic', 'url': 'http://1041jackfm.cbslocal.com/2017/02/15/carpool-karaoke-series-epic/'}\n",
            "{'date': '2017-01-01 00:00:00', 'description': '\"S--- happens,\" said Carey.', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/01/mariah-carey-getty.jpg?w=946', 'text': 'By Brian Ives\\nSo, you’re Mariah Carey, one of the world’s biggest stars for a quarter of a century, and you fail on national TV, big time. How do you handle it?\\nAs it turns out, you handle it with humor.\\nRelated: Mariah Carey Shades Ariana Grande, Demi Lovato\\nFor those just waking up now: in case you were totally absent from social media on New Year’s Eve (and if so, good for you!), Mariah Carey had a disastrous performance—or perhaps, non-performance is a better way to describe it—on Dick Clark’s New Year’s Rockin’ Eve last night. As CBS News reported, Carey “paced the stage without singing. It’s unclear what exactly were the technical difficulties, but the disaster was obvious as it unfolded live.”\\nPredictably, twitter erupted with mockery, including memes about 2016 claiming its final victim (Mariah Carey’s career).\\nCarey’s response was a single [NSFW] tweet, “S— happens. Have a happy and healthy new year everybody! Here’s to making more headlines in 2017.”', 'title': 'Mariah Carey Reacts to Her NYE Performance with Humor', 'url': 'http://1041jackfm.cbslocal.com/2017/01/01/mariah-carey-reacts-to-her-nye-performance-with-humor/'}\n",
            "{'date': '2017-10-06 00:00:00', 'description': 'Dubbed the \"Raw Sessions Versions,\" the tracks are prototypes for the songs we know.', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/10/queen-photo-by-rogers-express-getty-images.jpg?w=946', 'text': 'By Hayden Wright\\nLast month, Queen announced a deluxe box set celebrating the 40th anniversary of their 1977 album News of the World. In the press release, the band promised “Every lead vocal is different, as are most of the lead guitar parts and a great many other instrumental details.” Now Queen have revealed the reissue versions of “We Are the Champions” and “We Will Rock You,” two of the band’s best-loved songs.\\nRelated: Queen Detail ‘News of the World’ Deluxe Box Set\\nDubbed the “Raw Sessions Versions,” the tracks are prototypes for the songs we know: Freddie Mercury’s vocals are a bit looser on “We Will Rock You,” which begins with a few warmup bars of singing. Brian May’s guitar solo is quite different, too. You get the sense that Queen were feeling their way through the tracks as they recorded earlier versions. The piano arrangement on “We Are the Champions” is brighter and happier.\\nThe News of the World box set debuts November 17. Listen to the never-before-heard raw sessions here:', 'title': 'Queen Release Alternate Takes of Classic Songs \" 104.1 Jack FM', 'url': 'http://1041jackfm.cbslocal.com/2017/10/06/queen-studio-outtakes-we-will-rock-you-we-are-the-champions/'}\n",
            "{'date': '2017-02-14 00:00:00', 'description': 'Katy Perry, Paul McCarteny and Miley Cyrus are just a few of the artists to spread the love.', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/02/grammy-red-carpet-43.jpg?w=946', 'text': \"By Radio.com Staff\\nIt’s Valentine’s day and artists are taking to social media to show love for their fans and significant others.\\nRelated: John Mayer is Cupid’s Secret Weapon\\nKaty Perry, Paul McCartney and Miley Cyrus are just a few of the musicians to spread the love.\\nChris Young had his tongue planted firmly in his cheek when he wished fans a “Happy Taco Tuesday,” and then there was Kesha, who loves her fans, but worries her cats will eat her. Valid concern.\\nCheck out the best Valentine’s Day messages below.\\n❤✨So much love for my #KatyCats on this mushy day! Thanks for keeping me floating and grounded all at the same time… twitter.com/i/web/status/8… —\\nKATY PERRY (@katyperry) February 14, 2017\\nAll we need is love. Happy Valentine's Day. X #ValentinesDay https://t.co/DsxieopcYJ —\\nPaul McCartney (@PaulMcCartney) February 14, 2017\\nHappy Valentine's Day https://t.co/p4VIPntyHx —\\nKim Kardashian West (@KimKardashian) February 14, 2017\\nHave a Happy Hippie Valentimezzzzz! TBTuesday to my date night with @tywrent!!!!!! It's all about L-O-V-E everyday.… twitter.com/i/web/status/8… —\\nMiley Ray Cyrus (@MileyCyrus) February 14, 2017\\nHappy Valentine's Day to all our fans ❤️ https://t.co/IzYZEVsyEX —\\nPearl Jam (@PearlJam) February 14, 2017\\nHappy Valentine's Day ❤😘😍 I love you —\\nAustin Mahone (@AustinMahone) February 14, 2017\\nHappy Valentines Day X Adam https://t.co/eiKiysyNE3 —\\n(@U2) February 14, 2017\\nHappy Valentine's Day, ya creeps. —\\nMark Hoppus (@markhoppus) February 14, 2017\\nI think I'm forgetting something today... Oh yeah! Happy Taco Tuesday everybody! —\\n(@ChrisYoungMusic) February 14, 2017\", 'title': 'Musicians Wish Fans a Happy Valentine’s Day', 'url': 'http://1041jackfm.cbslocal.com/2017/02/14/musicians-happy-valentines-day/'}\n",
            "{'date': '2017-08-14 00:00:00', 'description': \"The lineup for the four tour dates will feature Don Henley, Joe Walsh and Timothy B. Schmit with Frey's son Deacon and Vince Gill.\", 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/08/the-eagles-kennedy-center.jpg?w=946', 'text': \"By Annie Reuter\\nThe Eagles have announced four new concert dates. The lineup for the short run will feature Don Henley, Joe Walsh and Timothy B. Schmit with Frey’s son, Deacon, and Vince Gill filling in for the late Glenn Frey.\\nRelated: Vince Gill to Join Eagles for Classic East and West Shows\\nAn Evening with the Eagles will stop at the Greensboro Coliseum in North Carolina on October 17 followed by a show a Philips Arena in Atlanta, Georgia. on October 20. The band will return to Louisville, Kentucky on October 24 at KFC Yum! Center before traveling to the late Glenn Frey’s hometown of Detroit, Michigan on October 27 to wrap up the run at Little Caesars Arena.\\nTickets for the four new dates go on sale at Saturday (Aug. 19) at 10 am. An American Express card member pre-sale starts on Tuesday while VIP packages will be available through Eagles.com.\\nThe four new tour dates follow the success of the band’s Classic West and East shows, earlier this summer. Deacon and Gill also played with the band for those shows.\\n“Bringing Deacon in was my idea,” Don Henley told the LA Times. “I think of the guild system, which in both Eastern and Western cultures is a centuries-old tradition of the father passing down the trade to his son, and to me, that makes perfect moral and ethical sense. The primary thing is I think Glenn would be good with it—with both of these guys. I think he’d go, ‘That’s the perfect way to do this.'”\", 'title': 'The Eagles Add Four New Dates to 2017 Tour \" 104.1 Jack FM', 'url': 'http://1041jackfm.cbslocal.com/2017/08/14/the-eagles-2017-tour-dates/'}\n",
            "{'date': '2017-02-14 00:00:00', 'description': \"George Michael's manager, Michael Lippman, wanted the three artists as well as host James Corden to perform a mashup of Michael's hits.\", 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/02/grammy-show-60.jpg?w=946', 'text': 'By Annie Reuter\\nAdele’s tribute to George Michael on Sunday (Feb. 12) at the GRAMMY Awards was a memorable one despite an early glitch that caused her to start the song over. But the original plans for the performance were far different. At one point the tribute could have included Beyoncé and Rihanna.\\nRelated: Adele Tributes George Michael after Rocky Start at 2017 GRAMMYs\\nGeorge Michael’s manager, Michael Lippman, wanted the three artists as well as host James Corden to perform a mashup of Michael’s hits including “Freedom” and “One More Try,” GRAMMY executive producer Ken Ehrlich told Billboard,\\nThat decision shifted when Lippman found out “how passionate Adele was,” Ehrlich says, “and that she had a vision for what she wanted to do with it.” Adele would go on to perform a ballad version of Michael’s 1996 hit “Fastlove” backed by an orchestra. It was a song she recalls hearing for the first time at the age of 10 and she instantly “heard the vulnerability in that song,” Ehrlich says.\\nBackstage following her emotional GRAMMY performance, Adele raved about Michael, adding that “it was an honor” to show her respects to the singer. Later that night, Adele won GRAMMYs for Record of the Year and Album of the Year.', 'title': 'George Michael GRAMMY Tribute Plans Included Beyoncé, Rihanna', 'url': 'http://1041jackfm.cbslocal.com/2017/02/14/george-michael-grammy-tribute-plans-beyonce-rihanna/'}\n",
            "{'date': '2017-02-15 00:00:00', 'description': '\"In these trying times we need some fun. We’re very serious about fun.”', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/02/deborah-harry-getty.jpg?w=946', 'text': 'By Jon Wiederhorn\\nBlondie have released a video for “Fun,” the first single from their upcoming album Pollinator. Directed by Beyoncé collaborator Dikyal Rimmasch, the clip lives up to the song’s title, featuring the band in a space ship, wandering another planet and animated footage of galactic travel contrasted with performance shots and images of people partying on the dance floor.\\nRelated: Blondie and Garbage Announce Co-Headlining Summer Tour\\nGuitarist Chris Stein said the interstellar theme was an effort to escape the stress and turmoil of modern day events. “Quoting Emma Goldman, ‘If I can’t dance I don’t want to be part of your revolution.’ In these trying times we need some fun. We’re very serious about fun,” he told NME.\\n“The video was shot in two places,” he added. “The color stuff was all shot in LA without us and the black and white stuff was shot in New York. It’s got some good cameos—it’s got a pretty big cameo from Raja, who’s one of the Drag Race superstars, and little cameos from Grace McKagan, Duff McKagan’s daughter who has a band called The Pink Slips, and also Tony Maserati, who was a producer and mixer for us.”\\nStein and vocalist Debbie Harry wrote the upbeat, disco-inflected “Fun” with TV On The Radio member Dave Sitek. Pollinator, which is scheduled for release May 5, also features writing by Johnny Marr, Sia, Charli XCX, The Strokes guitarist Nick Valensi and Dev Hynes. It will be Blondie’s first record since 2014’s Ghost of Download.', 'title': 'Blondie Release Galactic Video for New Song ‘Fun’', 'url': 'http://1041jackfm.cbslocal.com/2017/02/15/blondie-video-new-song-fun/'}\n",
            "{'date': '2017-06-14 00:00:00', 'description': 'Check out the latest from the Las Vegas rockers.', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://s0.wp.com/wp-content/themes/vip/cbs-local/images/global/facebook/facebook-share-260x260.png', 'text': 'The Killers have released a new single titled “The Man” from their forthcoming album, Wonderful Wonderful.\\nRelated: The Killers Perform New Track, ‘Run for Cover’\\nThe track is the first new music from the band since their 2012 album Battle Born (assuming you don’t count their 2016 Christmas album Don’t Waste Your Wishes). Hopefully, this signals a fifth studio effort is imminent. “The Man” was recorded with the producer Jacknife Lee during album sessions in Las Vegas and Los Angeles. The song finds frontman Brandon Flowers looking back on his younger self, the persona from their Grammy-nominated debut Hot Fuss, and reconciling that wide-eyed character with the man he is now.\\nCheck out the latest from The Killers below.', 'title': 'Listen to The Killers’ New Track ‘The Man’', 'url': 'http://1041jackfm.cbslocal.com/2017/06/14/the-killers-the-man-3/'}\n",
            "{'date': '2017-06-14 00:00:00', 'description': \"The song is featured in the upcoming film 'The Book of Henry.'\", 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/06/stevienickspress.jpg?w=946', 'text': 'By Abby Hassler\\nStevie Nicks debuted a new ballad “Your Hand I Will Never Let Go” today (June 14). The track is featured in the Naomi Wats-drama, The Book of Henry, which will hit theaters this Friday (June 16).\\nRelated: Lana Del Rey Taps Stevie Nicks for New Track: Report\\nWritten by Thomas Barlett and Ryan Miller, Nicks’ song will fall alongside original music composed by Michael Giacchino on the film’s soundtrack.\\n“Drowned in thought and caught in a stare/ Talking to ghosts who were not there,” Nicks sings. “Then you took my hand/ Transformation began/ Commotion where it once was still/ Fireworks explode/ Front row tickets to the show/ This hand I will never let it go.”\\nListen to “Your Hand I Will Never Let It Go” below.', 'title': 'Stevie Nicks Debuts New Single ‘Your Hand I Will Never Let It Go’', 'url': 'http://1041jackfm.cbslocal.com/2017/06/14/stevie-nicks-your-hand-i-will-never-let-it-go/'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xYpQogNtm5g"
      },
      "source": [
        "## Create some samples using a form of self-training (pseudo-labeling)\n",
        "\n",
        "Maybe use `1` for language overlap between domains and `0` for lack of language overlap?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ens__PI9yYXZ",
        "outputId": "fbafdb92-acdd-4d66-8c7f-5d9f01e6cebe"
      },
      "source": [
        "bert = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=1)\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXSHqq2y1kJm"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bert = bert.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ox98B3Wiw8mH",
        "outputId": "8b8f2fed-3716-4dfd-a2b2-06be2a85f2b8"
      },
      "source": [
        "# create a temporary classifier that can distinguish between two domains in question (when they are probably not mixed)\n",
        "immigrant_domain_regex = re.compile(r'immigra.*?\\b|foreign.*?\\b|alien.*?\\b|naturalized.*?\\b')\n",
        "virus_domain_regex = re.compile(r'covid.*?\\b|coronavirus.*?\\b|virus.*?\\b|disease.*?\\b|infection.*?\\b|viral.*?\\b')\n",
        "\n",
        "min_length = 4_000\n",
        "optimizer = optim.AdamW(bert.parameters(), lr=3e-5)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# limit token counts to 50 for occurences in their own domain\n",
        "token_counts = Counter()\n",
        "\n",
        "for epoch in range(3):\n",
        "\n",
        "    running_loss = 0.\n",
        "    b = 0\n",
        "\n",
        "    for s in dataset['train']:\n",
        "        for paragraph in s['text'].split('\\n'):\n",
        "            sentences = re.findall(r'.*?[.?!]', paragraph)\n",
        "            for sentence in sentences:\n",
        "                if sentence.strip():\n",
        "                    loss = None\n",
        "                    if immigrant_domain_regex.search(sentence) and not virus_domain_regex.search(sentence):\n",
        "                        token = immigrant_domain_regex.search(sentence).group()\n",
        "                        if token_counts[token] < 50:\n",
        "                            output = bert(**bert_tokenizer(sentence, return_tensors='pt').to(device))\n",
        "                            loss = criterion(torch.sigmoid(output.logits), torch.tensor([[1.]]).to(device))\n",
        "                            token_counts[token] += 1\n",
        "                    elif virus_domain_regex.search(sentence) and not immigrant_domain_regex.search(sentence):\n",
        "                        token = virus_domain_regex.search(sentence).group()\n",
        "                        if token_counts[token] < 50:\n",
        "                            output = bert(**bert_tokenizer(sentence, return_tensors='pt').to(device))\n",
        "                            loss = criterion(torch.sigmoid(output.logits), torch.tensor([[0.]]).to(device))\n",
        "                            token_counts[token] += 1\n",
        "                    elif virus_domain_regex.search(sentence) and immigrant_domain_regex.search(sentence):\n",
        "                        output = bert(**bert_tokenizer(sentence, return_tensors='pt').to(device))\n",
        "                        loss = criterion(torch.sigmoid(output.logits), torch.tensor([[0.5]]).to(device))\n",
        "                    if loss:\n",
        "                        loss.backward()\n",
        "                        running_loss += loss.item()\n",
        "\n",
        "                        if b % 10 == 0:\n",
        "                            print(f'Epoch {epoch + 1} Batch {b + 1} Loss {loss.item()} Running Loss {running_loss / (b + 1)}')\n",
        "                            optimizer.step()\n",
        "                            optimizer.zero_grad()\n",
        "\n",
        "                        b += 1\n",
        "optimizer.zero_grad()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 1 Loss 1.1954939365386963 Running Loss 1.1954939365386963\n",
            "Epoch 1 Batch 11 Loss 0.8024701476097107 Running Loss 0.6667622788385912\n",
            "Epoch 1 Batch 21 Loss 0.5517479181289673 Running Loss 0.6972458277429853\n",
            "Epoch 1 Batch 31 Loss 0.9239817261695862 Running Loss 0.7240914798551991\n",
            "Epoch 1 Batch 41 Loss 0.5092869997024536 Running Loss 0.7064966014245662\n",
            "Epoch 1 Batch 51 Loss 0.46393609046936035 Running Loss 0.6661917543878743\n",
            "Epoch 1 Batch 61 Loss 0.4503921568393707 Running Loss 0.6361267776762853\n",
            "Epoch 1 Batch 71 Loss 0.4306190013885498 Running Loss 0.6073215196669941\n",
            "Epoch 1 Batch 81 Loss 0.4079379141330719 Running Loss 0.5966959706059208\n",
            "Epoch 1 Batch 91 Loss 0.42254960536956787 Running Loss 0.5809331316869337\n",
            "Epoch 1 Batch 101 Loss 0.3265847861766815 Running Loss 0.5642704332228934\n",
            "Epoch 1 Batch 111 Loss 0.3256513774394989 Running Loss 0.5416274529856604\n",
            "Epoch 1 Batch 121 Loss 0.3233015239238739 Running Loss 0.5282528326531087\n",
            "Epoch 1 Batch 131 Loss 0.32245543599128723 Running Loss 0.5152675761976315\n",
            "Epoch 1 Batch 141 Loss 1.0308136940002441 Running Loss 0.508868397761744\n",
            "Epoch 1 Batch 151 Loss 0.2664153575897217 Running Loss 0.524036115171104\n",
            "Epoch 1 Batch 161 Loss 0.32734960317611694 Running Loss 0.5150934229171054\n",
            "Epoch 1 Batch 171 Loss 0.24166272580623627 Running Loss 0.503608379360528\n",
            "Epoch 1 Batch 181 Loss 0.9991906881332397 Running Loss 0.5196187981090493\n",
            "Epoch 1 Batch 191 Loss 1.0041770935058594 Running Loss 0.5468104107217638\n",
            "Epoch 1 Batch 201 Loss 0.8704490661621094 Running Loss 0.5507928804675145\n",
            "Epoch 1 Batch 211 Loss 0.24201230704784393 Running Loss 0.5573314811804848\n",
            "Epoch 1 Batch 221 Loss 0.7890333533287048 Running Loss 0.5636098878416\n",
            "Epoch 1 Batch 231 Loss 0.8413940072059631 Running Loss 0.5683834621142515\n",
            "Epoch 1 Batch 241 Loss 0.20161506533622742 Running Loss 0.5688746144910076\n",
            "Epoch 1 Batch 251 Loss 0.31434908509254456 Running Loss 0.5579790401150031\n",
            "Epoch 1 Batch 261 Loss 0.2319117933511734 Running Loss 0.5480360551469627\n",
            "Epoch 1 Batch 271 Loss 0.18187707662582397 Running Loss 0.5349789173290739\n",
            "Epoch 1 Batch 281 Loss 0.17214983701705933 Running Loss 0.5307264514134872\n",
            "Epoch 1 Batch 291 Loss 0.14037495851516724 Running Loss 0.535517140403646\n",
            "Epoch 1 Batch 301 Loss 0.1572839766740799 Running Loss 0.5233624131477552\n",
            "Epoch 1 Batch 311 Loss 0.4118592143058777 Running Loss 0.5196459010195502\n",
            "Epoch 1 Batch 321 Loss 0.36411577463150024 Running Loss 0.5148904989255922\n",
            "Epoch 1 Batch 331 Loss 0.5893172025680542 Running Loss 0.5098254947593925\n",
            "Epoch 1 Batch 341 Loss 0.29995521903038025 Running Loss 0.502447122003326\n",
            "Epoch 1 Batch 351 Loss 0.35415491461753845 Running Loss 0.49495736779140953\n",
            "Epoch 1 Batch 361 Loss 0.1462848037481308 Running Loss 0.48745257611750237\n",
            "Epoch 1 Batch 371 Loss 0.0771196186542511 Running Loss 0.47826211415532466\n",
            "Epoch 1 Batch 381 Loss 0.10333669185638428 Running Loss 0.4745847774459308\n",
            "Epoch 1 Batch 391 Loss 0.11095049232244492 Running Loss 0.4650053480816314\n",
            "Epoch 1 Batch 401 Loss 0.11042643338441849 Running Loss 0.4550087094734286\n",
            "Epoch 1 Batch 411 Loss 0.08252229541540146 Running Loss 0.44783877195232974\n",
            "Epoch 1 Batch 421 Loss 0.09903764724731445 Running Loss 0.4390022575926045\n",
            "Epoch 1 Batch 431 Loss 0.06669443100690842 Running Loss 0.43033165497199705\n",
            "Epoch 1 Batch 441 Loss 0.023184258490800858 Running Loss 0.4221176608020013\n",
            "Epoch 1 Batch 451 Loss 0.06120038777589798 Running Loss 0.414201150291048\n",
            "Epoch 1 Batch 461 Loss 0.05328778177499771 Running Loss 0.4059113013859103\n",
            "Epoch 1 Batch 471 Loss 0.05992772430181503 Running Loss 0.3982086367509674\n",
            "Epoch 1 Batch 481 Loss 0.050341904163360596 Running Loss 0.3909292789195469\n",
            "Epoch 1 Batch 491 Loss 0.050368357449769974 Running Loss 0.38356323792293756\n",
            "Epoch 1 Batch 501 Loss 0.010894840583205223 Running Loss 0.3763733745335105\n",
            "Epoch 1 Batch 511 Loss 0.03874750807881355 Running Loss 0.36956220017855446\n",
            "Epoch 1 Batch 521 Loss 0.00846814177930355 Running Loss 0.36314350113562255\n",
            "Epoch 1 Batch 531 Loss 0.008932732045650482 Running Loss 0.3569344864279495\n",
            "Epoch 1 Batch 541 Loss 0.0415961816906929 Running Loss 0.35087308279959945\n",
            "Epoch 1 Batch 551 Loss 0.006527702324092388 Running Loss 0.344849770536271\n",
            "Epoch 1 Batch 561 Loss 2.7679190635681152 Running Loss 0.3440437595569155\n",
            "Epoch 1 Batch 571 Loss 0.020293133333325386 Running Loss 0.33848258823029137\n",
            "Epoch 1 Batch 581 Loss 0.04054737463593483 Running Loss 0.3376251736859408\n",
            "Epoch 1 Batch 591 Loss 0.025462068617343903 Running Loss 0.3322596156129743\n",
            "Epoch 1 Batch 601 Loss 0.024713849648833275 Running Loss 0.3270607049927153\n",
            "Epoch 1 Batch 611 Loss 0.007625313010066748 Running Loss 0.32186014117953504\n",
            "Epoch 1 Batch 621 Loss 0.0059553938917815685 Running Loss 0.3168787342153532\n",
            "Epoch 1 Batch 631 Loss 0.006371307652443647 Running Loss 0.3120167764053546\n",
            "Epoch 1 Batch 641 Loss 0.004658565856516361 Running Loss 0.30727146751924406\n",
            "Epoch 1 Batch 651 Loss 0.005124264396727085 Running Loss 0.3065663285119983\n",
            "Epoch 1 Batch 661 Loss 0.03279484063386917 Running Loss 0.30216788673708594\n",
            "Epoch 1 Batch 671 Loss 0.019784873351454735 Running Loss 0.29813930523219545\n",
            "Epoch 1 Batch 681 Loss 0.01867195963859558 Running Loss 0.2942156922851693\n",
            "Epoch 1 Batch 691 Loss 0.01505444385111332 Running Loss 0.29039661523073307\n",
            "Epoch 1 Batch 701 Loss 0.004746538121253252 Running Loss 0.28645647448300254\n",
            "Epoch 1 Batch 711 Loss 0.003969370853155851 Running Loss 0.2825750339142297\n",
            "Epoch 1 Batch 721 Loss 0.013154278509318829 Running Loss 0.2815084359208161\n",
            "Epoch 1 Batch 731 Loss 0.014330356381833553 Running Loss 0.2778138533284284\n",
            "Epoch 1 Batch 741 Loss 0.011677350848913193 Running Loss 0.27424428930164246\n",
            "Epoch 1 Batch 751 Loss 1.7445430755615234 Running Loss 0.27594080656585834\n",
            "Epoch 1 Batch 761 Loss 0.010814099572598934 Running Loss 0.27248125332719725\n",
            "Epoch 1 Batch 771 Loss 0.017857927829027176 Running Loss 0.2709042351957454\n",
            "Epoch 1 Batch 781 Loss 0.013540934771299362 Running Loss 0.2675786911725769\n",
            "Epoch 1 Batch 791 Loss 0.011647921986877918 Running Loss 0.2644568549671626\n",
            "Epoch 1 Batch 801 Loss 0.010373387485742569 Running Loss 0.261365116473878\n",
            "Epoch 1 Batch 811 Loss 1.019483208656311 Running Loss 0.25960212347181527\n",
            "Epoch 1 Batch 821 Loss 0.0118181724101305 Running Loss 0.25654686732449083\n",
            "Epoch 1 Batch 831 Loss 0.009788280352950096 Running Loss 0.25424496939172686\n",
            "Epoch 1 Batch 841 Loss 0.010749871842563152 Running Loss 0.25135501189485154\n",
            "Epoch 1 Batch 851 Loss 0.00884366873651743 Running Loss 0.2487985982759894\n",
            "Epoch 1 Batch 861 Loss 0.05046163499355316 Running Loss 0.2466131477274918\n",
            "Epoch 1 Batch 871 Loss 1.8229384422302246 Running Loss 0.24597411977242675\n",
            "Epoch 1 Batch 881 Loss 0.011401072144508362 Running Loss 0.24326751232633492\n",
            "Epoch 1 Batch 891 Loss 0.006699122488498688 Running Loss 0.24068361408193203\n",
            "Epoch 1 Batch 901 Loss 0.0065361615270376205 Running Loss 0.23816515889109735\n",
            "Epoch 1 Batch 911 Loss 0.004953593015670776 Running Loss 0.23589072305439235\n",
            "Epoch 1 Batch 921 Loss 0.006031067110598087 Running Loss 0.2336884058413301\n",
            "Epoch 1 Batch 931 Loss 0.0051807621493935585 Running Loss 0.23146724649359873\n",
            "Epoch 1 Batch 941 Loss 0.005178126040846109 Running Loss 0.22905879547793762\n",
            "Epoch 1 Batch 951 Loss 0.004594493191689253 Running Loss 0.2271807031675617\n",
            "Epoch 1 Batch 961 Loss 0.004017724189907312 Running Loss 0.22515643358959367\n",
            "Epoch 1 Batch 971 Loss 0.003627015510573983 Running Loss 0.22288029038454726\n",
            "Epoch 1 Batch 981 Loss 0.003650405677035451 Running Loss 0.22071714170166043\n",
            "Epoch 1 Batch 991 Loss 0.0035314257256686687 Running Loss 0.21856749219599994\n",
            "Epoch 1 Batch 1001 Loss 0.004994506016373634 Running Loss 0.21647715669257434\n",
            "Epoch 1 Batch 1011 Loss 0.043835531920194626 Running Loss 0.2144470156905537\n",
            "Epoch 1 Batch 1021 Loss 0.0026714690029621124 Running Loss 0.2123741167424161\n",
            "Epoch 1 Batch 1031 Loss 0.0035404579248279333 Running Loss 0.2103418842846461\n",
            "Epoch 1 Batch 1041 Loss 0.043898433446884155 Running Loss 0.2088396556997058\n",
            "Epoch 1 Batch 1051 Loss 0.0023095435462892056 Running Loss 0.20719398887691529\n",
            "Epoch 1 Batch 1061 Loss 0.0021555989515036345 Running Loss 0.2053096378933108\n",
            "Epoch 1 Batch 1071 Loss 0.002016071928665042 Running Loss 0.20343330555174144\n",
            "Epoch 1 Batch 1081 Loss 0.007084130775183439 Running Loss 0.20159222261987067\n",
            "Epoch 1 Batch 1091 Loss 0.001802577986381948 Running Loss 0.19976122644642413\n",
            "Epoch 1 Batch 1101 Loss 0.001636114320717752 Running Loss 0.19802180346138318\n",
            "Epoch 1 Batch 1111 Loss 0.0018996747676283121 Running Loss 0.1963130834368439\n",
            "Epoch 1 Batch 1121 Loss 0.0017048936570063233 Running Loss 0.19464126987055436\n",
            "Epoch 1 Batch 1131 Loss 0.0015080611919984221 Running Loss 0.19295788344257667\n",
            "Epoch 1 Batch 1141 Loss 0.0012690129224210978 Running Loss 0.1913579000494958\n",
            "Epoch 1 Batch 1151 Loss 0.04000924155116081 Running Loss 0.18980986799751057\n",
            "Epoch 1 Batch 1161 Loss 0.032069798558950424 Running Loss 0.18833596009358405\n",
            "Epoch 1 Batch 1171 Loss 0.0011978168040513992 Running Loss 0.18677201175640307\n",
            "Epoch 1 Batch 1181 Loss 0.001437325612641871 Running Loss 0.18522015616925044\n",
            "Epoch 1 Batch 1191 Loss 0.0013586567947641015 Running Loss 0.1836981043141565\n",
            "Epoch 1 Batch 1201 Loss 0.0009616467286832631 Running Loss 0.18232760562463163\n",
            "Epoch 1 Batch 1211 Loss 0.0009430920472368598 Running Loss 0.18313251354049662\n",
            "Epoch 1 Batch 1221 Loss 0.01955314725637436 Running Loss 0.18169209820044477\n",
            "Epoch 1 Batch 1231 Loss 0.0010370623786002398 Running Loss 0.18197959684175768\n",
            "Epoch 1 Batch 1241 Loss 0.001355672487989068 Running Loss 0.1819986582787255\n",
            "Epoch 1 Batch 1251 Loss 1.6088794469833374 Running Loss 0.18486278656994637\n",
            "Epoch 1 Batch 1261 Loss 0.0018122514011338353 Running Loss 0.18702517366504834\n",
            "Epoch 1 Batch 1271 Loss 0.0016056665917858481 Running Loss 0.18824893704784615\n",
            "Epoch 1 Batch 1281 Loss 0.003935800399631262 Running Loss 0.18691364382036504\n",
            "Epoch 1 Batch 1291 Loss 0.11103887856006622 Running Loss 0.18597680597693575\n",
            "Epoch 1 Batch 1301 Loss 0.011147834360599518 Running Loss 0.18505909906582674\n",
            "Epoch 1 Batch 1311 Loss 0.02965901419520378 Running Loss 0.18393518983486445\n",
            "Epoch 1 Batch 1321 Loss 0.06503766775131226 Running Loss 0.1844136671682296\n",
            "Epoch 1 Batch 1331 Loss 0.12227042764425278 Running Loss 0.1848275355688488\n",
            "Epoch 1 Batch 1341 Loss 0.010644321329891682 Running Loss 0.18385528257677658\n",
            "Epoch 1 Batch 1351 Loss 0.6995026469230652 Running Loss 0.18453750065933716\n",
            "Epoch 1 Batch 1361 Loss 0.010696793906390667 Running Loss 0.18336329470462323\n",
            "Epoch 1 Batch 1371 Loss 0.13491927087306976 Running Loss 0.1840654068903194\n",
            "Epoch 1 Batch 1381 Loss 0.002928607165813446 Running Loss 0.18380244002217402\n",
            "Epoch 1 Batch 1391 Loss 0.13608983159065247 Running Loss 0.18326448255644892\n",
            "Epoch 1 Batch 1401 Loss 0.09611974656581879 Running Loss 0.18435318108300228\n",
            "Epoch 1 Batch 1411 Loss 0.0029533563647419214 Running Loss 0.18333906502063432\n",
            "Epoch 1 Batch 1421 Loss 0.014820000156760216 Running Loss 0.18327922404776992\n",
            "Epoch 1 Batch 1431 Loss 0.025439200922846794 Running Loss 0.18212570999920658\n",
            "Epoch 1 Batch 1441 Loss 0.13415153324604034 Running Loss 0.18123529800487745\n",
            "Epoch 1 Batch 1451 Loss 0.004564134869724512 Running Loss 0.1805516837013845\n",
            "Epoch 1 Batch 1461 Loss 0.13359126448631287 Running Loss 0.18150399293485822\n",
            "Epoch 1 Batch 1471 Loss 0.105061836540699 Running Loss 0.18283308633880305\n",
            "Epoch 1 Batch 1481 Loss 0.02367245964705944 Running Loss 0.18247378655770796\n",
            "Epoch 1 Batch 1491 Loss 0.07049430906772614 Running Loss 0.18306626157289668\n",
            "Epoch 1 Batch 1501 Loss 0.08901131898164749 Running Loss 0.18342137409988846\n",
            "Epoch 1 Batch 1511 Loss 0.04408198595046997 Running Loss 0.1826808768212299\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-787f720dc3ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                             \u001b[0mtoken_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0;32melif\u001b[0m \u001b[0mvirus_domain_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mimmigrant_domain_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvirus_domain_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mtoken_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JCnow78Btq9R",
        "outputId": "9221f1f5-74c8-48c2-f694-989c4c29b35b"
      },
      "source": [
        "# now look for when the model is ambiguous about how to classify a sentence\n",
        "i = 1\n",
        "samples = []\n",
        "for j, s in enumerate(dataset['train']):\n",
        "    if j < 1_511:\n",
        "        continue\n",
        "    for paragraph in s['text'].split('\\n'):\n",
        "        sentences = re.findall(r'.*?[.?!]', paragraph)\n",
        "        for sentence in sentences:\n",
        "            if sentence.strip() and re.search(r'immigra|covid|coronavirus|foreign|alien|naturalized|virus|disease|infection', sentence):\n",
        "                with torch.no_grad():\n",
        "                    try:\n",
        "                        target = bert(**bert_tokenizer(sentence, return_tensors='pt').to(device))\n",
        "                    except:\n",
        "                        continue\n",
        "                    sigmoid = torch.sigmoid(target.logits).item()\n",
        "                if sigmoid <= 0.3:\n",
        "                    samples.append((sentence, 0, sigmoid))\n",
        "                elif sigmoid >= 0.7:\n",
        "                    samples.append((sentence, 1, sigmoid))\n",
        "                else:\n",
        "                    samples.append((sentence, 2, sigmoid))\n",
        "                i += 1\n",
        "                if i % 500 == 0:\n",
        "                    print('On sample', i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On sample 500\n",
            "On sample 1000\n",
            "On sample 1500\n",
            "On sample 2000\n",
            "On sample 2500\n",
            "On sample 3000\n",
            "On sample 3500\n",
            "On sample 4000\n",
            "On sample 4500\n",
            "On sample 5000\n",
            "On sample 5500\n",
            "On sample 6000\n",
            "On sample 6500\n",
            "On sample 7000\n",
            "On sample 7500\n",
            "On sample 8000\n",
            "On sample 8500\n",
            "On sample 9000\n",
            "On sample 9500\n",
            "On sample 10000\n",
            "On sample 10500\n",
            "On sample 11000\n",
            "On sample 11500\n",
            "On sample 12000\n",
            "On sample 12500\n",
            "On sample 13000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "On sample 13500\n",
            "On sample 14000\n",
            "On sample 14500\n",
            "On sample 15000\n",
            "On sample 15500\n",
            "On sample 16000\n",
            "On sample 16500\n",
            "On sample 17000\n",
            "On sample 17500\n",
            "On sample 18000\n",
            "On sample 18500\n",
            "On sample 19000\n",
            "On sample 19500\n",
            "On sample 20000\n",
            "On sample 20500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-d5fcfdf28727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'.*?[.?!]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'immigra|covid|coronavirus|foreign|alien|naturalized|virus|disease|infection'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/re.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"Scan through string looking for a match to the pattern, returning\n\u001b[1;32m    184\u001b[0m     a Match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "rgbVy2bHvfEb",
        "outputId": "46029742-f6d0-4462-e3ad-3eeaef17a0bf"
      },
      "source": [
        "df = pd.DataFrame(data=samples, columns=['text', 'target', 'sigmoid'])\n",
        "df[df['target'] == 2].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>sigmoid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>157.0</td>\n",
              "      <td>157.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.532379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.100923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.310892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.447861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.523684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.629069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.699679</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       target     sigmoid\n",
              "count   157.0  157.000000\n",
              "mean      2.0    0.532379\n",
              "std       0.0    0.100923\n",
              "min       2.0    0.310892\n",
              "25%       2.0    0.447861\n",
              "50%       2.0    0.523684\n",
              "75%       2.0    0.629069\n",
              "max       2.0    0.699679"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "iKoj-8DjZtg9",
        "outputId": "8259e7a2-6329-4d29-e1b5-857823577f65"
      },
      "source": [
        "df[df['target'] == 2].iloc[3].text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Trump also related the opioid crisis to immigration, saying he will work to end sanctuary city policies and accusing Democrats of stonewalling progress on DACA because they want to stop construction of the border wall.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRsurUSxvoe4"
      },
      "source": [
        "# you can download from the directory (we can concatenate all of our efforts together)\n",
        "df[['target', 'sigmoid', 'text']].sort_values(by=['target', 'sigmoid', 'text'], ascending=[False, False, True]).to_csv('immigration.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3woPdzvv3V8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}