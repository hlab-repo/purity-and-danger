{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Immigration Pseudo-Labeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hlab-repo/purity-and-danger/blob/master/Immigration_Pseudo_Labeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGNecY0ujs2w"
      },
      "source": [
        "# Creating a Model for Immigration and \n",
        "\n",
        "---\n",
        "\n",
        "Outsider Language\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "This notebook starts with a baseline system and then provides users the opportunity to attempt to improve performance with their own custom, complete system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BiHtT8qkT38"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMeyDaZAn70Q"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9fKFwGsP22I"
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import datasets\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YKxr_AzwQNh"
      },
      "source": [
        "We can start with the Common Crawl news corpus (January 2017 - December 2019). See here for details:\n",
        "\n",
        "https://huggingface.co/datasets/cc_news"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH8YRQzskXCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f239aed-05ed-4661-b824-421e46eace50"
      },
      "source": [
        "# this could take several minutes\n",
        "dataset = datasets.load_dataset('cc_news')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset cc_news (/root/.cache/huggingface/datasets/cc_news/plain_text/1.0.0/6cdde8d7fdaae3e50fb61b5d08d5387c2f0bbea1ee68755ef954af539a6a3a1b)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlx6xoDidiTa",
        "outputId": "57bda109-feb5-4524-ddda-bd7345f87b47"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['title', 'text', 'domain', 'date', 'description', 'url', 'image_url'],\n",
              "        num_rows: 708241\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEif50ojdmoa",
        "outputId": "52c2f626-40c9-4cde-af7f-4779cbe16ff2"
      },
      "source": [
        "# look at the first 10 samples\n",
        "for i, s in enumerate(dataset['train']):\n",
        "    print(s)\n",
        "    if i >= 10:\n",
        "        break"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'date': '2017-04-17 00:00:00', 'description': \"Officials unsealed court documents Monday (April 17) to reveal details surrounding the first searches of Prince's Paisley Park estate.\", 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/04/prince-young-and-sad.jpg?w=946', 'text': 'By Abby Hassler\\nOfficials unsealed court documents Monday (April 17) to reveal details surrounding the first searches of Prince’s Paisley Park estate following his untimely death.\\nRelated: Prince’s Ex-Wife Mayte Garcia Says Memoir is not a Tell-All\\nThe unsealed search warrants don’t confirm the source of the drug, fentanyl, that led to the 57-year-old singer’s accidental, self-administered overdose last April, according to The Star Tribune.\\nInvestigators found no prescriptions in Prince’s name, however, Dr. Michael Todd Schulenberg told detectives he had written a prescription for oxycodone, which is also an opioid, under the name of long-time Prince associate and drummer Kirk Johnson.\\nBetween April 21 and Sept. 19, 2016, Carver County authorities conducted investigations into Prince’s death with a total of 11 search warrants.', 'title': 'Prince Search Warrants Unsealed, Answer Few Questions', 'url': 'http://1041jackfm.cbslocal.com/2017/04/17/prince-search-warrants-unsealed-2/'}\n",
            "{'date': '2017-08-14 00:00:00', 'description': '\"The spirit of Green Day has always been about rising above oppression.\"', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/08/billie-joe-armstrong-theo-wargo-getty-images.jpg?w=946', 'text': 'By Abby Hassler\\nGreen Day’s Billie Joe Armstrong has always been outspoken about his political beliefs. Following the tragedy in Charlottesville, Virgina, over the weekend, Armstrong felt the need to speak out against the white supremacists who caused much of the violence.\\nRelated: Billie Joe Armstrong Wins #TBT with Childhood Studio Photo\\n“My heart feels heavy. I feel like what happened in Charlottesville goes beyond the point of anger,” Armstrong wrote on Facebook. “It makes me sad and desperate. shocked. I f—— hate racism more than anything.”\\n“The spirit of Green Day has always been about rising above oppression. and sticking up for what you believe in and singing it at the top of your lungs,” Armstrong continued. “We grew up fearing nuclear holocaust because of the cold war. those days are feeling way too relevant these days. these issues are our ugly past.. and now it’s coming to haunt us. always resist these doomsday politicians. and in the words of our punk forefathers .. Nazi punks f— off.”', 'title': 'Green Day’s Billie Joe Armstrong Rails Against White Nationalists', 'url': 'http://1041jackfm.cbslocal.com/2017/08/14/billie-joe-armstrong-white-nationalists/'}\n",
            "{'date': '2017-02-15 00:00:00', 'description': 'The concept, which has broken YouTube records and dominated social media, remains as simple and delightful as ever. Only this time, a rotating cast of celebrities will replace James Corden.', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/02/metallica-carpool.jpg?w=946', 'text': 'By Hayden Wright\\nA trailer has been released for the next sequence of Carpool Karaoke videos, and the lineup is stellar. The CBS late-night segment will stream on Apple Music in a similar format, but with new guests and even crazier moments behind the wheel. The concept, which has broken YouTube records and dominated social media, remains as simple and delightful as ever. Only this time, a rotating cast of celebrities will replace James Corden.\\nRelated: Adele’s ‘Carpool Karaoke’ is the Most Popular Viral Video of 2016\\nHere are the moments we’re most excited for:\\nMetallica making their headbanging mark on Rihanna’s “Diamonds” with Billy Eichner.\\nJohn Legend duetting on with Alicia Keys on her breakout hit, “Falling.”\\nAriana Grande belting the Little Shop of Horrors soundtrack with Seth MacFarlane.\\nJames Corden’s return with Will Smith — they rap the Fresh Prince theme!\\nChelsea Handler slinging whiskey and singing Bon Jovi’s “Living on a Prayer” with Blake Shelton.\\nCorden also presides over an epic, R. Kelly-inspired key change for the Carpool Karaoke franchise — there’s a helicopter.\\nA new installment will drop on Apple Music each week and the premiere is “coming soon.”\\nWatch the preview here:', 'title': 'Upcoming ‘Carpool Karaoke’ Series Looks Epic', 'url': 'http://1041jackfm.cbslocal.com/2017/02/15/carpool-karaoke-series-epic/'}\n",
            "{'date': '2017-01-01 00:00:00', 'description': '\"S--- happens,\" said Carey.', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/01/mariah-carey-getty.jpg?w=946', 'text': 'By Brian Ives\\nSo, you’re Mariah Carey, one of the world’s biggest stars for a quarter of a century, and you fail on national TV, big time. How do you handle it?\\nAs it turns out, you handle it with humor.\\nRelated: Mariah Carey Shades Ariana Grande, Demi Lovato\\nFor those just waking up now: in case you were totally absent from social media on New Year’s Eve (and if so, good for you!), Mariah Carey had a disastrous performance—or perhaps, non-performance is a better way to describe it—on Dick Clark’s New Year’s Rockin’ Eve last night. As CBS News reported, Carey “paced the stage without singing. It’s unclear what exactly were the technical difficulties, but the disaster was obvious as it unfolded live.”\\nPredictably, twitter erupted with mockery, including memes about 2016 claiming its final victim (Mariah Carey’s career).\\nCarey’s response was a single [NSFW] tweet, “S— happens. Have a happy and healthy new year everybody! Here’s to making more headlines in 2017.”', 'title': 'Mariah Carey Reacts to Her NYE Performance with Humor', 'url': 'http://1041jackfm.cbslocal.com/2017/01/01/mariah-carey-reacts-to-her-nye-performance-with-humor/'}\n",
            "{'date': '2017-10-06 00:00:00', 'description': 'Dubbed the \"Raw Sessions Versions,\" the tracks are prototypes for the songs we know.', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/10/queen-photo-by-rogers-express-getty-images.jpg?w=946', 'text': 'By Hayden Wright\\nLast month, Queen announced a deluxe box set celebrating the 40th anniversary of their 1977 album News of the World. In the press release, the band promised “Every lead vocal is different, as are most of the lead guitar parts and a great many other instrumental details.” Now Queen have revealed the reissue versions of “We Are the Champions” and “We Will Rock You,” two of the band’s best-loved songs.\\nRelated: Queen Detail ‘News of the World’ Deluxe Box Set\\nDubbed the “Raw Sessions Versions,” the tracks are prototypes for the songs we know: Freddie Mercury’s vocals are a bit looser on “We Will Rock You,” which begins with a few warmup bars of singing. Brian May’s guitar solo is quite different, too. You get the sense that Queen were feeling their way through the tracks as they recorded earlier versions. The piano arrangement on “We Are the Champions” is brighter and happier.\\nThe News of the World box set debuts November 17. Listen to the never-before-heard raw sessions here:', 'title': 'Queen Release Alternate Takes of Classic Songs \" 104.1 Jack FM', 'url': 'http://1041jackfm.cbslocal.com/2017/10/06/queen-studio-outtakes-we-will-rock-you-we-are-the-champions/'}\n",
            "{'date': '2017-02-14 00:00:00', 'description': 'Katy Perry, Paul McCarteny and Miley Cyrus are just a few of the artists to spread the love.', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/02/grammy-red-carpet-43.jpg?w=946', 'text': \"By Radio.com Staff\\nIt’s Valentine’s day and artists are taking to social media to show love for their fans and significant others.\\nRelated: John Mayer is Cupid’s Secret Weapon\\nKaty Perry, Paul McCartney and Miley Cyrus are just a few of the musicians to spread the love.\\nChris Young had his tongue planted firmly in his cheek when he wished fans a “Happy Taco Tuesday,” and then there was Kesha, who loves her fans, but worries her cats will eat her. Valid concern.\\nCheck out the best Valentine’s Day messages below.\\n❤✨So much love for my #KatyCats on this mushy day! Thanks for keeping me floating and grounded all at the same time… twitter.com/i/web/status/8… —\\nKATY PERRY (@katyperry) February 14, 2017\\nAll we need is love. Happy Valentine's Day. X #ValentinesDay https://t.co/DsxieopcYJ —\\nPaul McCartney (@PaulMcCartney) February 14, 2017\\nHappy Valentine's Day https://t.co/p4VIPntyHx —\\nKim Kardashian West (@KimKardashian) February 14, 2017\\nHave a Happy Hippie Valentimezzzzz! TBTuesday to my date night with @tywrent!!!!!! It's all about L-O-V-E everyday.… twitter.com/i/web/status/8… —\\nMiley Ray Cyrus (@MileyCyrus) February 14, 2017\\nHappy Valentine's Day to all our fans ❤️ https://t.co/IzYZEVsyEX —\\nPearl Jam (@PearlJam) February 14, 2017\\nHappy Valentine's Day ❤😘😍 I love you —\\nAustin Mahone (@AustinMahone) February 14, 2017\\nHappy Valentines Day X Adam https://t.co/eiKiysyNE3 —\\n(@U2) February 14, 2017\\nHappy Valentine's Day, ya creeps. —\\nMark Hoppus (@markhoppus) February 14, 2017\\nI think I'm forgetting something today... Oh yeah! Happy Taco Tuesday everybody! —\\n(@ChrisYoungMusic) February 14, 2017\", 'title': 'Musicians Wish Fans a Happy Valentine’s Day', 'url': 'http://1041jackfm.cbslocal.com/2017/02/14/musicians-happy-valentines-day/'}\n",
            "{'date': '2017-08-14 00:00:00', 'description': \"The lineup for the four tour dates will feature Don Henley, Joe Walsh and Timothy B. Schmit with Frey's son Deacon and Vince Gill.\", 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/08/the-eagles-kennedy-center.jpg?w=946', 'text': \"By Annie Reuter\\nThe Eagles have announced four new concert dates. The lineup for the short run will feature Don Henley, Joe Walsh and Timothy B. Schmit with Frey’s son, Deacon, and Vince Gill filling in for the late Glenn Frey.\\nRelated: Vince Gill to Join Eagles for Classic East and West Shows\\nAn Evening with the Eagles will stop at the Greensboro Coliseum in North Carolina on October 17 followed by a show a Philips Arena in Atlanta, Georgia. on October 20. The band will return to Louisville, Kentucky on October 24 at KFC Yum! Center before traveling to the late Glenn Frey’s hometown of Detroit, Michigan on October 27 to wrap up the run at Little Caesars Arena.\\nTickets for the four new dates go on sale at Saturday (Aug. 19) at 10 am. An American Express card member pre-sale starts on Tuesday while VIP packages will be available through Eagles.com.\\nThe four new tour dates follow the success of the band’s Classic West and East shows, earlier this summer. Deacon and Gill also played with the band for those shows.\\n“Bringing Deacon in was my idea,” Don Henley told the LA Times. “I think of the guild system, which in both Eastern and Western cultures is a centuries-old tradition of the father passing down the trade to his son, and to me, that makes perfect moral and ethical sense. The primary thing is I think Glenn would be good with it—with both of these guys. I think he’d go, ‘That’s the perfect way to do this.'”\", 'title': 'The Eagles Add Four New Dates to 2017 Tour \" 104.1 Jack FM', 'url': 'http://1041jackfm.cbslocal.com/2017/08/14/the-eagles-2017-tour-dates/'}\n",
            "{'date': '2017-02-14 00:00:00', 'description': \"George Michael's manager, Michael Lippman, wanted the three artists as well as host James Corden to perform a mashup of Michael's hits.\", 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/02/grammy-show-60.jpg?w=946', 'text': 'By Annie Reuter\\nAdele’s tribute to George Michael on Sunday (Feb. 12) at the GRAMMY Awards was a memorable one despite an early glitch that caused her to start the song over. But the original plans for the performance were far different. At one point the tribute could have included Beyoncé and Rihanna.\\nRelated: Adele Tributes George Michael after Rocky Start at 2017 GRAMMYs\\nGeorge Michael’s manager, Michael Lippman, wanted the three artists as well as host James Corden to perform a mashup of Michael’s hits including “Freedom” and “One More Try,” GRAMMY executive producer Ken Ehrlich told Billboard,\\nThat decision shifted when Lippman found out “how passionate Adele was,” Ehrlich says, “and that she had a vision for what she wanted to do with it.” Adele would go on to perform a ballad version of Michael’s 1996 hit “Fastlove” backed by an orchestra. It was a song she recalls hearing for the first time at the age of 10 and she instantly “heard the vulnerability in that song,” Ehrlich says.\\nBackstage following her emotional GRAMMY performance, Adele raved about Michael, adding that “it was an honor” to show her respects to the singer. Later that night, Adele won GRAMMYs for Record of the Year and Album of the Year.', 'title': 'George Michael GRAMMY Tribute Plans Included Beyoncé, Rihanna', 'url': 'http://1041jackfm.cbslocal.com/2017/02/14/george-michael-grammy-tribute-plans-beyonce-rihanna/'}\n",
            "{'date': '2017-02-15 00:00:00', 'description': '\"In these trying times we need some fun. We’re very serious about fun.”', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/02/deborah-harry-getty.jpg?w=946', 'text': 'By Jon Wiederhorn\\nBlondie have released a video for “Fun,” the first single from their upcoming album Pollinator. Directed by Beyoncé collaborator Dikyal Rimmasch, the clip lives up to the song’s title, featuring the band in a space ship, wandering another planet and animated footage of galactic travel contrasted with performance shots and images of people partying on the dance floor.\\nRelated: Blondie and Garbage Announce Co-Headlining Summer Tour\\nGuitarist Chris Stein said the interstellar theme was an effort to escape the stress and turmoil of modern day events. “Quoting Emma Goldman, ‘If I can’t dance I don’t want to be part of your revolution.’ In these trying times we need some fun. We’re very serious about fun,” he told NME.\\n“The video was shot in two places,” he added. “The color stuff was all shot in LA without us and the black and white stuff was shot in New York. It’s got some good cameos—it’s got a pretty big cameo from Raja, who’s one of the Drag Race superstars, and little cameos from Grace McKagan, Duff McKagan’s daughter who has a band called The Pink Slips, and also Tony Maserati, who was a producer and mixer for us.”\\nStein and vocalist Debbie Harry wrote the upbeat, disco-inflected “Fun” with TV On The Radio member Dave Sitek. Pollinator, which is scheduled for release May 5, also features writing by Johnny Marr, Sia, Charli XCX, The Strokes guitarist Nick Valensi and Dev Hynes. It will be Blondie’s first record since 2014’s Ghost of Download.', 'title': 'Blondie Release Galactic Video for New Song ‘Fun’', 'url': 'http://1041jackfm.cbslocal.com/2017/02/15/blondie-video-new-song-fun/'}\n",
            "{'date': '2017-06-14 00:00:00', 'description': 'Check out the latest from the Las Vegas rockers.', 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://s0.wp.com/wp-content/themes/vip/cbs-local/images/global/facebook/facebook-share-260x260.png', 'text': 'The Killers have released a new single titled “The Man” from their forthcoming album, Wonderful Wonderful.\\nRelated: The Killers Perform New Track, ‘Run for Cover’\\nThe track is the first new music from the band since their 2012 album Battle Born (assuming you don’t count their 2016 Christmas album Don’t Waste Your Wishes). Hopefully, this signals a fifth studio effort is imminent. “The Man” was recorded with the producer Jacknife Lee during album sessions in Las Vegas and Los Angeles. The song finds frontman Brandon Flowers looking back on his younger self, the persona from their Grammy-nominated debut Hot Fuss, and reconciling that wide-eyed character with the man he is now.\\nCheck out the latest from The Killers below.', 'title': 'Listen to The Killers’ New Track ‘The Man’', 'url': 'http://1041jackfm.cbslocal.com/2017/06/14/the-killers-the-man-3/'}\n",
            "{'date': '2017-06-14 00:00:00', 'description': \"The song is featured in the upcoming film 'The Book of Henry.'\", 'domain': '1041jackfm.cbslocal.com', 'image_url': 'https://cbs1041jackfm.files.wordpress.com/2017/06/stevienickspress.jpg?w=946', 'text': 'By Abby Hassler\\nStevie Nicks debuted a new ballad “Your Hand I Will Never Let Go” today (June 14). The track is featured in the Naomi Wats-drama, The Book of Henry, which will hit theaters this Friday (June 16).\\nRelated: Lana Del Rey Taps Stevie Nicks for New Track: Report\\nWritten by Thomas Barlett and Ryan Miller, Nicks’ song will fall alongside original music composed by Michael Giacchino on the film’s soundtrack.\\n“Drowned in thought and caught in a stare/ Talking to ghosts who were not there,” Nicks sings. “Then you took my hand/ Transformation began/ Commotion where it once was still/ Fireworks explode/ Front row tickets to the show/ This hand I will never let it go.”\\nListen to “Your Hand I Will Never Let It Go” below.', 'title': 'Stevie Nicks Debuts New Single ‘Your Hand I Will Never Let It Go’', 'url': 'http://1041jackfm.cbslocal.com/2017/06/14/stevie-nicks-your-hand-i-will-never-let-it-go/'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xYpQogNtm5g"
      },
      "source": [
        "## Create some samples using a form of self-training (pseudo-labeling)\n",
        "\n",
        "Maybe use `1` for language overlap between domains and `0` for lack of language overlap?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ens__PI9yYXZ",
        "outputId": "ac034f30-0783-453f-a952-4ad0ff9a48da"
      },
      "source": [
        "bert = BertForSequenceClassification.from_pretrained('bert-large-uncased', num_labels=1)\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXSHqq2y1kJm"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "bert = bert.to(device)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ox98B3Wiw8mH",
        "outputId": "7a06b01c-964d-43a2-9bec-a0f3925500c1"
      },
      "source": [
        "# create a temporary classifier that can distinguish between two domains in question (when they are probably not mixed)\n",
        "immigrant_domain_regex = re.compile(r'immigra.*?\\b|foreign.*?\\b|border\\scrossing.*?\\b|border\\scontrol.*?\\b|undocumented.*?\\b|alien.*?\\b|naturaliz.*?\\b')\n",
        "virus_domain_regex = re.compile(r'covid.*?\\b|coronavirus.*?\\b|virus.*?\\b|disease.*?\\b|infect.*?\\b||epidem.*?\\b|immun.*?\\b|pandem.*?\\bviral.*?\\b')\n",
        "\n",
        "min_length = 4_000\n",
        "optimizer = optim.AdamW(bert.parameters(), lr=3e-5)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# limit token counts to 50 for occurences in their own domain\n",
        "token_counts = Counter()\n",
        "\n",
        "for epoch in range(3):\n",
        "\n",
        "    running_loss = 0.\n",
        "    b = 0\n",
        "\n",
        "    for s in dataset['train']:\n",
        "        for paragraph in s['text'].split('\\n'):\n",
        "            sentences = re.findall(r'.*?[.?!]', paragraph)\n",
        "            for sentence in sentences:\n",
        "                if sentence.strip():\n",
        "                    loss = None\n",
        "                    if immigrant_domain_regex.search(sentence) and not virus_domain_regex.search(sentence):\n",
        "                        token = immigrant_domain_regex.search(sentence).group()\n",
        "                        if token_counts[token] < 50:\n",
        "                            output = bert(**bert_tokenizer(sentence, return_tensors='pt').to(device))\n",
        "                            loss = criterion(torch.sigmoid(output.logits), torch.tensor([[1.]]).to(device))\n",
        "                            token_counts[token] += 1\n",
        "                    elif virus_domain_regex.search(sentence) and not immigrant_domain_regex.search(sentence):\n",
        "                        token = virus_domain_regex.search(sentence).group()\n",
        "                        if token_counts[token] < 50:\n",
        "                            output = bert(**bert_tokenizer(sentence, return_tensors='pt').to(device))\n",
        "                            loss = criterion(torch.sigmoid(output.logits), torch.tensor([[0.]]).to(device))\n",
        "                            token_counts[token] += 1\n",
        "                    elif virus_domain_regex.search(sentence) and immigrant_domain_regex.search(sentence):\n",
        "                        output = bert(**bert_tokenizer(sentence, return_tensors='pt').to(device))\n",
        "                        loss = criterion(torch.sigmoid(output.logits), torch.tensor([[0.5]]).to(device))\n",
        "                    if loss:\n",
        "                        loss.backward()\n",
        "                        running_loss += loss.item()\n",
        "\n",
        "                        if b % 10 == 0:\n",
        "                            print(f'Epoch {epoch + 1} Batch {b + 1} Loss {loss.item()} Running Loss {running_loss / (b + 1)}')\n",
        "                            optimizer.step()\n",
        "                            optimizer.zero_grad()\n",
        "\n",
        "                        b += 1\n",
        "optimizer.zero_grad()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 1 Loss 0.6065901517868042 Running Loss 0.6065901517868042\n",
            "Epoch 1 Batch 11 Loss 0.6792342066764832 Running Loss 0.6032024188475176\n",
            "Epoch 1 Batch 21 Loss 0.4643443524837494 Running Loss 0.5677666720889863\n",
            "Epoch 1 Batch 31 Loss 0.48582637310028076 Running Loss 0.5401253604119823\n",
            "Epoch 1 Batch 41 Loss 0.4327050745487213 Running Loss 0.5150380367186012\n",
            "Epoch 1 Batch 51 Loss 0.7478040456771851 Running Loss 0.5027365684509277\n",
            "Epoch 1 Batch 61 Loss 0.7384192943572998 Running Loss 0.5441865422686593\n",
            "Epoch 1 Batch 71 Loss 0.7446213364601135 Running Loss 0.5765529644321388\n",
            "Epoch 1 Batch 81 Loss 0.7387065887451172 Running Loss 0.5975312965887564\n",
            "Epoch 1 Batch 91 Loss 0.7355107069015503 Running Loss 0.6140055807082208\n",
            "Epoch 1 Batch 101 Loss 0.7321497797966003 Running Loss 0.6259762119538713\n",
            "Epoch 1 Batch 111 Loss 0.7328019142150879 Running Loss 0.6356392239665126\n",
            "Epoch 1 Batch 121 Loss 0.734526515007019 Running Loss 0.6437633879913771\n",
            "Epoch 1 Batch 131 Loss 0.7334743738174438 Running Loss 0.6505356940604349\n",
            "Epoch 1 Batch 141 Loss 0.7298874855041504 Running Loss 0.6562320463200833\n",
            "Epoch 1 Batch 151 Loss 0.7292449474334717 Running Loss 0.6610763787433801\n",
            "Epoch 1 Batch 161 Loss 0.7270582318305969 Running Loss 0.6651172734195401\n",
            "Epoch 1 Batch 171 Loss 0.7242891788482666 Running Loss 0.6685590869502017\n",
            "Epoch 1 Batch 181 Loss 0.7222082614898682 Running Loss 0.671524741043702\n",
            "Epoch 1 Batch 191 Loss 0.7197726964950562 Running Loss 0.6740511587152931\n",
            "Epoch 1 Batch 201 Loss 0.7180290222167969 Running Loss 0.6762421765730748\n",
            "Epoch 1 Batch 211 Loss 0.7160872220993042 Running Loss 0.6781418504308185\n",
            "Epoch 1 Batch 221 Loss 0.7143320441246033 Running Loss 0.6797795635542718\n",
            "Epoch 1 Batch 231 Loss 0.7125594019889832 Running Loss 0.681202460419048\n",
            "Epoch 1 Batch 241 Loss 0.710936963558197 Running Loss 0.6824348618380757\n",
            "Epoch 1 Batch 251 Loss 0.7094463109970093 Running Loss 0.683508716256495\n",
            "Epoch 1 Batch 261 Loss 0.7079828381538391 Running Loss 0.6844387086415199\n",
            "Epoch 1 Batch 271 Loss 0.7060730457305908 Running Loss 0.6852396508864371\n",
            "Epoch 1 Batch 281 Loss 0.7052013874053955 Running Loss 0.6859283082425807\n",
            "Epoch 1 Batch 291 Loss 0.703561544418335 Running Loss 0.6865243497992709\n",
            "Epoch 1 Batch 301 Loss 0.7018768787384033 Running Loss 0.6870319605665746\n",
            "Epoch 1 Batch 311 Loss 0.7006805539131165 Running Loss 0.6874618879085185\n",
            "Epoch 1 Batch 321 Loss 0.6996574401855469 Running Loss 0.6878349786606904\n",
            "Epoch 1 Batch 331 Loss 0.6980500221252441 Running Loss 0.6881519363365864\n",
            "Epoch 1 Batch 341 Loss 0.6972169876098633 Running Loss 0.6884192039540087\n",
            "Epoch 1 Batch 351 Loss 0.6963058710098267 Running Loss 0.6886417439520529\n",
            "Epoch 1 Batch 361 Loss 0.6951970458030701 Running Loss 0.688825548520709\n",
            "Epoch 1 Batch 371 Loss 0.6945809125900269 Running Loss 0.6889815505624138\n",
            "Epoch 1 Batch 381 Loss 0.6940038204193115 Running Loss 0.6891184555576855\n",
            "Epoch 1 Batch 391 Loss 0.6936935186386108 Running Loss 0.6892346922698838\n",
            "Epoch 1 Batch 401 Loss 0.6932999491691589 Running Loss 0.6893364906013755\n",
            "Epoch 1 Batch 411 Loss 0.6931763887405396 Running Loss 0.6894295634724508\n",
            "Epoch 1 Batch 421 Loss 0.6931560039520264 Running Loss 0.6895186813313717\n",
            "Epoch 1 Batch 431 Loss 0.6932886838912964 Running Loss 0.6896064743365047\n",
            "Epoch 1 Batch 441 Loss 0.6936584711074829 Running Loss 0.6896961297578011\n",
            "Epoch 1 Batch 451 Loss 0.6939435005187988 Running Loss 0.6897864196358657\n",
            "Epoch 1 Batch 461 Loss 0.6938486099243164 Running Loss 0.6898773816816202\n",
            "Epoch 1 Batch 471 Loss 0.6941555738449097 Running Loss 0.6899643935215701\n",
            "Epoch 1 Batch 481 Loss 0.6941162347793579 Running Loss 0.6900468458752622\n",
            "Epoch 1 Batch 491 Loss 0.6940392255783081 Running Loss 0.6901253294313512\n",
            "Epoch 1 Batch 501 Loss 0.6939133405685425 Running Loss 0.6901990007497594\n",
            "Epoch 1 Batch 511 Loss 0.6936590671539307 Running Loss 0.6902637320721919\n",
            "Epoch 1 Batch 521 Loss 0.6935855746269226 Running Loss 0.6903242399047295\n",
            "Epoch 1 Batch 531 Loss 0.6933706402778625 Running Loss 0.6903814514029049\n",
            "Epoch 1 Batch 541 Loss 0.6932483911514282 Running Loss 0.6904338002204895\n",
            "Epoch 1 Batch 551 Loss 0.6931730508804321 Running Loss 0.690483757783194\n",
            "Epoch 1 Batch 561 Loss 0.6933811902999878 Running Loss 0.6905318248930674\n",
            "Epoch 1 Batch 571 Loss 0.6932361125946045 Running Loss 0.6905788513073362\n",
            "Epoch 1 Batch 581 Loss 0.6934475302696228 Running Loss 0.6906272591494037\n",
            "Epoch 1 Batch 591 Loss 0.6932058334350586 Running Loss 0.690673054071449\n",
            "Epoch 1 Batch 601 Loss 0.6935057640075684 Running Loss 0.6907184211267607\n",
            "Epoch 1 Batch 611 Loss 0.6931832432746887 Running Loss 0.6907601512590524\n",
            "Epoch 1 Batch 621 Loss 0.6931605935096741 Running Loss 0.6908003354802031\n",
            "Epoch 1 Batch 631 Loss 0.6931479573249817 Running Loss 0.6908383591428234\n",
            "Epoch 1 Batch 641 Loss 0.693259596824646 Running Loss 0.6908750337855121\n",
            "Epoch 1 Batch 651 Loss 0.6931670904159546 Running Loss 0.6909102184065659\n",
            "Epoch 1 Batch 661 Loss 0.6931568384170532 Running Loss 0.6909443587290177\n",
            "Epoch 1 Batch 671 Loss 0.6931560039520264 Running Loss 0.690977483410416\n",
            "Epoch 1 Batch 681 Loss 0.6931472420692444 Running Loss 0.6910094876996571\n",
            "Epoch 1 Batch 691 Loss 0.693153977394104 Running Loss 0.6910409268352988\n",
            "Epoch 1 Batch 701 Loss 0.6931569576263428 Running Loss 0.6910711300696184\n",
            "Epoch 1 Batch 711 Loss 0.6931767463684082 Running Loss 0.6911005922678318\n",
            "Epoch 1 Batch 721 Loss 0.6931612491607666 Running Loss 0.6911292715145381\n",
            "Epoch 1 Batch 731 Loss 0.69315505027771 Running Loss 0.6911571262310043\n",
            "Epoch 1 Batch 741 Loss 0.6931626200675964 Running Loss 0.6911842787635792\n",
            "Epoch 1 Batch 751 Loss 0.6931762099266052 Running Loss 0.6912107029545324\n",
            "Epoch 1 Batch 761 Loss 0.6931654810905457 Running Loss 0.6912363975182783\n",
            "Epoch 1 Batch 771 Loss 0.6931524276733398 Running Loss 0.6912613555924283\n",
            "Epoch 1 Batch 781 Loss 0.6931650042533875 Running Loss 0.6912856442369664\n",
            "Epoch 1 Batch 791 Loss 0.6931544542312622 Running Loss 0.6913093574882005\n",
            "Epoch 1 Batch 801 Loss 0.693149209022522 Running Loss 0.6913324097420244\n",
            "Epoch 1 Batch 811 Loss 0.6931574940681458 Running Loss 0.6913548544067637\n",
            "Epoch 1 Batch 821 Loss 0.6931473016738892 Running Loss 0.691376720073017\n",
            "Epoch 1 Batch 831 Loss 0.693152666091919 Running Loss 0.6913981094830877\n",
            "Epoch 1 Batch 841 Loss 0.6931488513946533 Running Loss 0.6914189454349695\n",
            "Epoch 1 Batch 851 Loss 0.6931496262550354 Running Loss 0.6914392647395823\n",
            "Epoch 1 Batch 861 Loss 0.6931471824645996 Running Loss 0.6914591372497683\n",
            "Epoch 1 Batch 871 Loss 0.6931475400924683 Running Loss 0.6914785406482753\n",
            "Epoch 1 Batch 881 Loss 0.6931477785110474 Running Loss 0.6914974962541924\n",
            "Epoch 1 Batch 891 Loss 0.6931473016738892 Running Loss 0.691516030316401\n",
            "Epoch 1 Batch 901 Loss 0.6931487321853638 Running Loss 0.6915341534307609\n",
            "Epoch 1 Batch 911 Loss 0.6931477785110474 Running Loss 0.6915518901874153\n",
            "Epoch 1 Batch 921 Loss 0.6931555271148682 Running Loss 0.6915692455346628\n",
            "Epoch 1 Batch 931 Loss 0.6931475400924683 Running Loss 0.6915862471281394\n",
            "Epoch 1 Batch 941 Loss 0.6931474208831787 Running Loss 0.6916028515818775\n",
            "Epoch 1 Batch 951 Loss 0.6931536197662354 Running Loss 0.691619195584619\n",
            "Epoch 1 Batch 961 Loss 0.6931498050689697 Running Loss 0.6916351421938727\n",
            "Epoch 1 Batch 971 Loss 0.693152666091919 Running Loss 0.691650767896006\n",
            "Epoch 1 Batch 981 Loss 0.6931474208831787 Running Loss 0.6916660661605034\n",
            "Epoch 1 Batch 991 Loss 0.6931507587432861 Running Loss 0.6916810364944542\n",
            "Epoch 1 Batch 1001 Loss 0.6931501626968384 Running Loss 0.6916957070658376\n",
            "Epoch 1 Batch 1011 Loss 0.6931544542312622 Running Loss 0.6917101022161904\n",
            "Epoch 1 Batch 1021 Loss 0.6931500434875488 Running Loss 0.691724202600212\n",
            "Epoch 1 Batch 1031 Loss 0.6931476593017578 Running Loss 0.6917380150606283\n",
            "Epoch 1 Batch 1041 Loss 0.6931478977203369 Running Loss 0.6917515799016705\n",
            "Epoch 1 Batch 1051 Loss 0.6931554079055786 Running Loss 0.6917648860435051\n",
            "Epoch 1 Batch 1061 Loss 0.6931484937667847 Running Loss 0.6917779170376978\n",
            "Epoch 1 Batch 1071 Loss 0.6931501626968384 Running Loss 0.6917907283419654\n",
            "Epoch 1 Batch 1081 Loss 0.6931473016738892 Running Loss 0.6918032848096136\n",
            "Epoch 1 Batch 1091 Loss 0.6931472420692444 Running Loss 0.6918156097287109\n",
            "Epoch 1 Batch 1101 Loss 0.6931474208831787 Running Loss 0.6918277107077658\n",
            "Epoch 1 Batch 1111 Loss 0.6931501030921936 Running Loss 0.691839600553607\n",
            "Epoch 1 Batch 1121 Loss 0.693149745464325 Running Loss 0.6918512839062952\n",
            "Epoch 1 Batch 1131 Loss 0.6931483149528503 Running Loss 0.6918627552286278\n",
            "Epoch 1 Batch 1141 Loss 0.6931471824645996 Running Loss 0.6918740234387537\n",
            "Epoch 1 Batch 1151 Loss 0.6931473016738892 Running Loss 0.6918850952288464\n",
            "Epoch 1 Batch 1161 Loss 0.6931471824645996 Running Loss 0.6918959746475778\n",
            "Epoch 1 Batch 1171 Loss 0.6931474804878235 Running Loss 0.6919066936515723\n",
            "Epoch 1 Batch 1181 Loss 0.6931472420692444 Running Loss 0.6919172037768223\n",
            "Epoch 1 Batch 1191 Loss 0.6931494474411011 Running Loss 0.6919275485198103\n",
            "Epoch 1 Batch 1201 Loss 0.6931473016738892 Running Loss 0.6919377147903252\n",
            "Epoch 1 Batch 1211 Loss 0.6931478977203369 Running Loss 0.6919477122269023\n",
            "Epoch 1 Batch 1221 Loss 0.6931480765342712 Running Loss 0.6919575381923366\n",
            "Epoch 1 Batch 1231 Loss 0.6931473016738892 Running Loss 0.6919672230605087\n",
            "Epoch 1 Batch 1241 Loss 0.6931479573249817 Running Loss 0.6919767498297619\n",
            "Epoch 1 Batch 1251 Loss 0.6931488513946533 Running Loss 0.6919861145728498\n",
            "Epoch 1 Batch 1261 Loss 0.6931487917900085 Running Loss 0.6919953297944791\n",
            "Epoch 1 Batch 1271 Loss 0.6931474804878235 Running Loss 0.6920044042762108\n",
            "Epoch 1 Batch 1281 Loss 0.6931472420692444 Running Loss 0.6920133401973465\n",
            "Epoch 1 Batch 1291 Loss 0.6931482553482056 Running Loss 0.692022132236398\n",
            "Epoch 1 Batch 1301 Loss 0.6931507587432861 Running Loss 0.6920307853146758\n",
            "Epoch 1 Batch 1311 Loss 0.6931473016738892 Running Loss 0.6920393052490316\n",
            "Epoch 1 Batch 1321 Loss 0.6931487917900085 Running Loss 0.6920477064336276\n",
            "Epoch 1 Batch 1331 Loss 0.6931471824645996 Running Loss 0.6920559846486121\n",
            "Epoch 1 Batch 1341 Loss 0.6931474804878235 Running Loss 0.6920641289550274\n",
            "Epoch 1 Batch 1351 Loss 0.6931475400924683 Running Loss 0.6920721518561013\n",
            "Epoch 1 Batch 1361 Loss 0.6932122707366943 Running Loss 0.6920801052094206\n",
            "Epoch 1 Batch 1371 Loss 0.6931471824645996 Running Loss 0.6920878922826265\n",
            "Epoch 1 Batch 1381 Loss 0.6931484937667847 Running Loss 0.6920955704658641\n",
            "Epoch 1 Batch 1391 Loss 0.6931654810905457 Running Loss 0.6921031494778236\n",
            "Epoch 1 Batch 1401 Loss 0.6931478977203369 Running Loss 0.6921106105953518\n",
            "Epoch 1 Batch 1411 Loss 0.6931486129760742 Running Loss 0.6921179593243048\n",
            "Epoch 1 Batch 1421 Loss 0.6931473016738892 Running Loss 0.6921252063006939\n",
            "Epoch 1 Batch 1431 Loss 0.6931476593017578 Running Loss 0.692132349659062\n",
            "Epoch 1 Batch 1441 Loss 0.6931518316268921 Running Loss 0.6921393988365766\n",
            "Epoch 1 Batch 1451 Loss 0.693147599697113 Running Loss 0.6921463477291131\n",
            "Epoch 1 Batch 1461 Loss 0.6931474804878235 Running Loss 0.692153204352296\n",
            "Epoch 1 Batch 1471 Loss 0.6931525468826294 Running Loss 0.6921599696964572\n",
            "Epoch 1 Batch 1481 Loss 0.6931484341621399 Running Loss 0.69216664484592\n",
            "Epoch 1 Batch 1491 Loss 0.6931472420692444 Running Loss 0.692173230656036\n",
            "Epoch 1 Batch 1501 Loss 0.6931477785110474 Running Loss 0.6921797302625403\n",
            "Epoch 1 Batch 1511 Loss 0.6931486129760742 Running Loss 0.6921861527535712\n",
            "Epoch 1 Batch 1521 Loss 0.6931489109992981 Running Loss 0.6921924745307636\n",
            "Epoch 1 Batch 1531 Loss 0.6931484937667847 Running Loss 0.692198714697618\n",
            "Epoch 1 Batch 1541 Loss 0.6931501626968384 Running Loss 0.6922048763127238\n",
            "Epoch 1 Batch 1551 Loss 0.6931477785110474 Running Loss 0.6922109622789305\n",
            "Epoch 1 Batch 1561 Loss 0.693148136138916 Running Loss 0.6922169714917898\n",
            "Epoch 1 Batch 1571 Loss 0.6931486129760742 Running Loss 0.6922229007882422\n",
            "Epoch 1 Batch 1581 Loss 0.6931472420692444 Running Loss 0.6922287586593386\n",
            "Epoch 1 Batch 1591 Loss 0.693147599697113 Running Loss 0.6922345365989441\n",
            "Epoch 1 Batch 1601 Loss 0.6931471824645996 Running Loss 0.6922402462312983\n",
            "Epoch 1 Batch 1611 Loss 0.6931480765342712 Running Loss 0.6922458895313895\n",
            "Epoch 1 Batch 1621 Loss 0.6931473612785339 Running Loss 0.6922514593799787\n",
            "Epoch 1 Batch 1631 Loss 0.6931490898132324 Running Loss 0.6922569646563433\n",
            "Epoch 1 Batch 1641 Loss 0.6931495070457458 Running Loss 0.6922624004751644\n",
            "Epoch 1 Batch 1651 Loss 0.6931472420692444 Running Loss 0.6922677619250741\n",
            "Epoch 1 Batch 1661 Loss 0.6931474208831787 Running Loss 0.6922730630166124\n",
            "Epoch 1 Batch 1671 Loss 0.6931478381156921 Running Loss 0.692278300267647\n",
            "Epoch 1 Batch 1681 Loss 0.6931474804878235 Running Loss 0.6922834751366292\n",
            "Epoch 1 Batch 1691 Loss 0.6931477785110474 Running Loss 0.6922885905279343\n",
            "Epoch 1 Batch 1701 Loss 0.6931480765342712 Running Loss 0.6922936444770301\n",
            "Epoch 1 Batch 1711 Loss 0.6931473612785339 Running Loss 0.6922986438440204\n",
            "Epoch 1 Batch 1721 Loss 0.6931471824645996 Running Loss 0.6923035787746423\n",
            "Epoch 1 Batch 1731 Loss 0.69315105676651 Running Loss 0.6923084594416935\n",
            "Epoch 1 Batch 1741 Loss 0.6931472420692444 Running Loss 0.6923132948256443\n",
            "Epoch 1 Batch 1751 Loss 0.6931473016738892 Running Loss 0.6923180609209615\n",
            "Epoch 1 Batch 1761 Loss 0.693148136138916 Running Loss 0.6923227836840671\n",
            "Epoch 1 Batch 1771 Loss 0.6931474208831787 Running Loss 0.6923274505549\n",
            "Epoch 1 Batch 1781 Loss 0.6931471824645996 Running Loss 0.6923320583585003\n",
            "Epoch 1 Batch 1791 Loss 0.6931475400924683 Running Loss 0.6923366139748455\n",
            "Epoch 1 Batch 1801 Loss 0.6931471824645996 Running Loss 0.6923411189682414\n",
            "Epoch 1 Batch 1811 Loss 0.6931512355804443 Running Loss 0.692345576876109\n",
            "Epoch 1 Batch 1821 Loss 0.6931471824645996 Running Loss 0.6923499874922026\n",
            "Epoch 1 Batch 1831 Loss 0.6931473016738892 Running Loss 0.6923543444948077\n",
            "Epoch 1 Batch 1841 Loss 0.6931502223014832 Running Loss 0.6923586557832249\n",
            "Epoch 1 Batch 1851 Loss 0.6931471824645996 Running Loss 0.6923629193612526\n",
            "Epoch 1 Batch 1861 Loss 0.693149209022522 Running Loss 0.692367137247103\n",
            "Epoch 1 Batch 1871 Loss 0.6931473016738892 Running Loss 0.692371312594554\n",
            "Epoch 1 Batch 1881 Loss 0.6931474208831787 Running Loss 0.6923754593275254\n",
            "Epoch 1 Batch 1891 Loss 0.6931524276733398 Running Loss 0.692379547388449\n",
            "Epoch 1 Batch 1901 Loss 0.6931478381156921 Running Loss 0.6923835962963757\n",
            "Epoch 1 Batch 1911 Loss 0.6931500434875488 Running Loss 0.6923876050752473\n",
            "Epoch 1 Batch 1921 Loss 0.6931498050689697 Running Loss 0.6923915635850639\n",
            "Epoch 1 Batch 1931 Loss 0.6931474208831787 Running Loss 0.6923954828547295\n",
            "Epoch 1 Batch 1941 Loss 0.6931517124176025 Running Loss 0.6923993635214463\n",
            "Epoch 1 Batch 1951 Loss 0.6931473016738892 Running Loss 0.6924032074313846\n",
            "Epoch 1 Batch 1961 Loss 0.6931536197662354 Running Loss 0.6924070137790853\n",
            "Epoch 1 Batch 1971 Loss 0.6931477785110474 Running Loss 0.692410773217285\n",
            "Epoch 1 Batch 1981 Loss 0.6931472420692444 Running Loss 0.6924145033057443\n",
            "Epoch 1 Batch 1991 Loss 0.6931475400924683 Running Loss 0.6924181930507505\n",
            "Epoch 1 Batch 2001 Loss 0.6931473016738892 Running Loss 0.6924218564912833\n",
            "Epoch 1 Batch 2011 Loss 0.693149745464325 Running Loss 0.692425468114998\n",
            "Epoch 1 Batch 2021 Loss 0.6931477189064026 Running Loss 0.6924290441452189\n",
            "Epoch 1 Batch 2031 Loss 0.6931474804878235 Running Loss 0.6924325866631132\n",
            "Epoch 1 Batch 2041 Loss 0.6931471824645996 Running Loss 0.6924360918683319\n",
            "Epoch 1 Batch 2051 Loss 0.6931472420692444 Running Loss 0.6924395601613413\n",
            "Epoch 1 Batch 2061 Loss 0.6931473016738892 Running Loss 0.6924429956366286\n",
            "Epoch 1 Batch 2071 Loss 0.6931500434875488 Running Loss 0.6924464032593584\n",
            "Epoch 1 Batch 2081 Loss 0.6931485533714294 Running Loss 0.6924497789914994\n",
            "Epoch 1 Batch 2091 Loss 0.6931483745574951 Running Loss 0.6924531162782812\n",
            "Epoch 1 Batch 2101 Loss 0.6931475400924683 Running Loss 0.6924564263356521\n",
            "Epoch 1 Batch 2111 Loss 0.6931471824645996 Running Loss 0.692459706303521\n",
            "Epoch 1 Batch 2121 Loss 0.6931471824645996 Running Loss 0.6924629502845001\n",
            "Epoch 1 Batch 2131 Loss 0.693148136138916 Running Loss 0.6924661642673821\n",
            "Epoch 1 Batch 2141 Loss 0.6931476593017578 Running Loss 0.6924693523194956\n",
            "Epoch 1 Batch 2151 Loss 0.6931473612785339 Running Loss 0.6924725103134446\n",
            "Epoch 1 Batch 2161 Loss 0.6931475400924683 Running Loss 0.6924756414798717\n",
            "Epoch 1 Batch 2171 Loss 0.6931493282318115 Running Loss 0.6924787393806717\n",
            "Epoch 1 Batch 2181 Loss 0.6931479573249817 Running Loss 0.6924818122075381\n",
            "Epoch 1 Batch 2191 Loss 0.6931490898132324 Running Loss 0.692484861908847\n",
            "Epoch 1 Batch 2201 Loss 0.6931474804878235 Running Loss 0.6924878772363399\n",
            "Epoch 1 Batch 2211 Loss 0.6931474208831787 Running Loss 0.6924908649376861\n",
            "Epoch 1 Batch 2221 Loss 0.6931498646736145 Running Loss 0.6924938263790074\n",
            "Epoch 1 Batch 2231 Loss 0.6931476593017578 Running Loss 0.6924967581463843\n",
            "Epoch 1 Batch 2241 Loss 0.6931501626968384 Running Loss 0.6924996703982832\n",
            "Epoch 1 Batch 2251 Loss 0.6931474208831787 Running Loss 0.6925025550273725\n",
            "Epoch 1 Batch 2261 Loss 0.6931480765342712 Running Loss 0.692505409368526\n",
            "Epoch 1 Batch 2271 Loss 0.6931471824645996 Running Loss 0.6925082408032822\n",
            "Epoch 1 Batch 2281 Loss 0.6931482553482056 Running Loss 0.6925110446157711\n",
            "Epoch 1 Batch 2291 Loss 0.6931477785110474 Running Loss 0.6925138252783634\n",
            "Epoch 1 Batch 2301 Loss 0.6931478977203369 Running Loss 0.6925165796476777\n",
            "Epoch 1 Batch 2311 Loss 0.6931484341621399 Running Loss 0.6925193162668014\n",
            "Epoch 1 Batch 2321 Loss 0.6931511759757996 Running Loss 0.6925220326173414\n",
            "Epoch 1 Batch 2331 Loss 0.6931482553482056 Running Loss 0.6925247173767548\n",
            "Epoch 1 Batch 2341 Loss 0.693148136138916 Running Loss 0.6925273822801127\n",
            "Epoch 1 Batch 2351 Loss 0.6931481957435608 Running Loss 0.6925300220285057\n",
            "Epoch 1 Batch 2361 Loss 0.6931478977203369 Running Loss 0.6925326415614929\n",
            "Epoch 1 Batch 2371 Loss 0.6931480765342712 Running Loss 0.6925352356042705\n",
            "Epoch 1 Batch 2381 Loss 0.6931475400924683 Running Loss 0.6925378091342306\n",
            "Epoch 1 Batch 2391 Loss 0.6931471824645996 Running Loss 0.6925403596167283\n",
            "Epoch 1 Batch 2401 Loss 0.6931480169296265 Running Loss 0.6925428912124253\n",
            "Epoch 1 Batch 2411 Loss 0.6931472420692444 Running Loss 0.692545399384991\n",
            "Epoch 1 Batch 2421 Loss 0.6931478381156921 Running Loss 0.69254788843771\n",
            "Epoch 1 Batch 2431 Loss 0.6931476593017578 Running Loss 0.6925503563753423\n",
            "Epoch 1 Batch 2441 Loss 0.6931473016738892 Running Loss 0.6925528052399169\n",
            "Epoch 1 Batch 2451 Loss 0.6931473016738892 Running Loss 0.6925552359214854\n",
            "Epoch 1 Batch 2461 Loss 0.6931471824645996 Running Loss 0.6925576460017561\n",
            "Epoch 1 Batch 2471 Loss 0.6931480765342712 Running Loss 0.6925600348624659\n",
            "Epoch 1 Batch 2481 Loss 0.6931474208831787 Running Loss 0.6925624082617969\n",
            "Epoch 1 Batch 2491 Loss 0.6931484341621399 Running Loss 0.6925647672234363\n",
            "Epoch 1 Batch 2501 Loss 0.6931490898132324 Running Loss 0.6925671025544632\n",
            "Epoch 1 Batch 2511 Loss 0.6931471824645996 Running Loss 0.6925694196882214\n",
            "Epoch 1 Batch 2521 Loss 0.6931474208831787 Running Loss 0.6925717190776918\n",
            "Epoch 1 Batch 2531 Loss 0.6931498050689697 Running Loss 0.6925739978717185\n",
            "Epoch 1 Batch 2541 Loss 0.6931473612785339 Running Loss 0.6925762559616017\n",
            "Epoch 1 Batch 2551 Loss 0.6931475400924683 Running Loss 0.692578528007775\n",
            "Epoch 1 Batch 2561 Loss 0.6931471824645996 Running Loss 0.692580752775847\n",
            "Epoch 1 Batch 2571 Loss 0.6931475400924683 Running Loss 0.6925829648507889\n",
            "Epoch 1 Batch 2581 Loss 0.6931512355804443 Running Loss 0.6925851581679532\n",
            "Epoch 1 Batch 2591 Loss 0.6931481957435608 Running Loss 0.6925873290567681\n",
            "Epoch 1 Batch 2601 Loss 0.6931490302085876 Running Loss 0.692589486254861\n",
            "Epoch 1 Batch 2611 Loss 0.6931554079055786 Running Loss 0.6925916304674152\n",
            "Epoch 1 Batch 2621 Loss 0.6931477189064026 Running Loss 0.692593753315119\n",
            "Epoch 1 Batch 2631 Loss 0.6931472420692444 Running Loss 0.6925958650323294\n",
            "Epoch 1 Batch 2641 Loss 0.6931473016738892 Running Loss 0.6925979583428602\n",
            "Epoch 1 Batch 2651 Loss 0.6931471824645996 Running Loss 0.6926000321959154\n",
            "Epoch 1 Batch 2661 Loss 0.6931474804878235 Running Loss 0.6926020943146368\n",
            "Epoch 1 Batch 2671 Loss 0.6931473016738892 Running Loss 0.6926041381584933\n",
            "Epoch 1 Batch 2681 Loss 0.6931472420692444 Running Loss 0.692606168689678\n",
            "Epoch 1 Batch 2691 Loss 0.6931471824645996 Running Loss 0.6926081835536964\n",
            "Epoch 1 Batch 2701 Loss 0.6931471824645996 Running Loss 0.6926101806295311\n",
            "Epoch 1 Batch 2711 Loss 0.693148136138916 Running Loss 0.6926121647751055\n",
            "Epoch 1 Batch 2721 Loss 0.6931471824645996 Running Loss 0.6926141342491141\n",
            "Epoch 1 Batch 2731 Loss 0.6931492686271667 Running Loss 0.6926160933376886\n",
            "Epoch 1 Batch 2741 Loss 0.6931489109992981 Running Loss 0.6926180341086305\n",
            "Epoch 1 Batch 2751 Loss 0.6931478977203369 Running Loss 0.6926199618316659\n",
            "Epoch 1 Batch 2761 Loss 0.6931480169296265 Running Loss 0.6926218741443527\n",
            "Epoch 1 Batch 2771 Loss 0.6931472420692444 Running Loss 0.6926237763329475\n",
            "Epoch 1 Batch 2781 Loss 0.6931538581848145 Running Loss 0.6926256642843999\n",
            "Epoch 1 Batch 2791 Loss 0.6931475400924683 Running Loss 0.6926275348629161\n",
            "Epoch 1 Batch 2801 Loss 0.693147599697113 Running Loss 0.6926293920636475\n",
            "Epoch 1 Batch 2811 Loss 0.6931475400924683 Running Loss 0.6926312353508365\n",
            "Epoch 1 Batch 2821 Loss 0.6931474804878235 Running Loss 0.6926330653584117\n",
            "Epoch 1 Batch 2831 Loss 0.6931473612785339 Running Loss 0.6926348820165542\n",
            "Epoch 1 Batch 2841 Loss 0.6931472420692444 Running Loss 0.6926366854662293\n",
            "Epoch 1 Batch 2851 Loss 0.6931471824645996 Running Loss 0.6926384760554904\n",
            "Epoch 1 Batch 2861 Loss 0.6931471824645996 Running Loss 0.6926402541275251\n",
            "Epoch 1 Batch 2871 Loss 0.6931471824645996 Running Loss 0.6926420198131297\n",
            "Epoch 1 Batch 2881 Loss 0.6931471824645996 Running Loss 0.6926437732412848\n",
            "Epoch 1 Batch 2891 Loss 0.6931472420692444 Running Loss 0.6926455147453593\n",
            "Epoch 1 Batch 2901 Loss 0.6931473016738892 Running Loss 0.6926472444486634\n",
            "Epoch 1 Batch 2911 Loss 0.6931473016738892 Running Loss 0.6926489622680562\n",
            "Epoch 1 Batch 2921 Loss 0.6931473016738892 Running Loss 0.6926506683255907\n",
            "Epoch 1 Batch 2931 Loss 0.6931472420692444 Running Loss 0.6926523625382951\n",
            "Epoch 1 Batch 2941 Loss 0.6931472420692444 Running Loss 0.6926540452296618\n",
            "Epoch 1 Batch 2951 Loss 0.6931471824645996 Running Loss 0.6926557163148361\n",
            "Epoch 1 Batch 2961 Loss 0.6931471824645996 Running Loss 0.6926573761127076\n",
            "Epoch 1 Batch 2971 Loss 0.6931471824645996 Running Loss 0.6926590247372512\n",
            "Epoch 1 Batch 2981 Loss 0.6931471824645996 Running Loss 0.6926606623009122\n",
            "Epoch 1 Batch 2991 Loss 0.6931471824645996 Running Loss 0.6926622889146323\n",
            "Epoch 1 Batch 3001 Loss 0.6931471824645996 Running Loss 0.6926639046878745\n",
            "Epoch 1 Batch 3011 Loss 0.6931471824645996 Running Loss 0.6926655097286474\n",
            "Epoch 1 Batch 3021 Loss 0.6931471824645996 Running Loss 0.6926671041435297\n",
            "Epoch 1 Batch 3031 Loss 0.6931475400924683 Running Loss 0.6926686917347155\n",
            "Epoch 1 Batch 3041 Loss 0.6931472420692444 Running Loss 0.6926702672382281\n",
            "Epoch 1 Batch 3051 Loss 0.6931477785110474 Running Loss 0.6926718350122577\n",
            "Epoch 1 Batch 3061 Loss 0.6931517124176025 Running Loss 0.6926733905176289\n",
            "Epoch 1 Batch 3071 Loss 0.6931493282318115 Running Loss 0.6926749370378378\n",
            "Epoch 1 Batch 3081 Loss 0.693153977394104 Running Loss 0.6926764782006691\n",
            "Epoch 1 Batch 3091 Loss 0.693148136138916 Running Loss 0.6926780023724467\n",
            "Epoch 1 Batch 3101 Loss 0.6931473016738892 Running Loss 0.6926795165602602\n",
            "Epoch 1 Batch 3111 Loss 0.6931496858596802 Running Loss 0.6926810234277357\n",
            "Epoch 1 Batch 3121 Loss 0.6931480169296265 Running Loss 0.6926825203142346\n",
            "Epoch 1 Batch 3131 Loss 0.6931488513946533 Running Loss 0.6926840076009445\n",
            "Epoch 1 Batch 3141 Loss 0.69315105676651 Running Loss 0.692685487182305\n",
            "Epoch 1 Batch 3151 Loss 0.6931502819061279 Running Loss 0.6926869565212549\n",
            "Epoch 1 Batch 3161 Loss 0.6931503415107727 Running Loss 0.6926884202782168\n",
            "Epoch 1 Batch 3171 Loss 0.6931535005569458 Running Loss 0.6926898739383786\n",
            "Epoch 1 Batch 3181 Loss 0.6931471824645996 Running Loss 0.6926913141867006\n",
            "Epoch 1 Batch 3191 Loss 0.6931487321853638 Running Loss 0.6926927456322302\n",
            "Epoch 1 Batch 3201 Loss 0.6931473612785339 Running Loss 0.6926941683574678\n",
            "Epoch 1 Batch 3211 Loss 0.693149983882904 Running Loss 0.6926955838360841\n",
            "Epoch 1 Batch 3221 Loss 0.6931471824645996 Running Loss 0.6926969890267314\n",
            "Epoch 1 Batch 3231 Loss 0.6931472420692444 Running Loss 0.692698385777469\n",
            "Epoch 1 Batch 3241 Loss 0.6931471824645996 Running Loss 0.6926997752882602\n",
            "Epoch 1 Batch 3251 Loss 0.6931477785110474 Running Loss 0.6927011539773991\n",
            "Epoch 1 Batch 3261 Loss 0.693147599697113 Running Loss 0.6927025257645484\n",
            "Epoch 1 Batch 3271 Loss 0.6931488513946533 Running Loss 0.6927038866494708\n",
            "Epoch 1 Batch 3281 Loss 0.6931471824645996 Running Loss 0.6927052393660099\n",
            "Epoch 1 Batch 3291 Loss 0.6931472420692444 Running Loss 0.692706583916181\n",
            "Epoch 1 Batch 3301 Loss 0.6931473612785339 Running Loss 0.6927079224687603\n",
            "Epoch 1 Batch 3311 Loss 0.6931471824645996 Running Loss 0.6927092535119156\n",
            "Epoch 1 Batch 3321 Loss 0.6931472420692444 Running Loss 0.6927105758212411\n",
            "Epoch 1 Batch 3331 Loss 0.6931486129760742 Running Loss 0.6927118894753962\n",
            "Epoch 1 Batch 3341 Loss 0.6931482553482056 Running Loss 0.6927131950873096\n",
            "Epoch 1 Batch 3351 Loss 0.6931473016738892 Running Loss 0.6927144932625935\n",
            "Epoch 1 Batch 3361 Loss 0.6931473016738892 Running Loss 0.6927157813010916\n",
            "Epoch 1 Batch 3371 Loss 0.6931504607200623 Running Loss 0.692717063289055\n",
            "Epoch 1 Batch 3381 Loss 0.6931491494178772 Running Loss 0.6927183382047841\n",
            "Epoch 1 Batch 3391 Loss 0.6931477785110474 Running Loss 0.6927196051440946\n",
            "Epoch 1 Batch 3401 Loss 0.693148672580719 Running Loss 0.6927208648608454\n",
            "Epoch 1 Batch 3411 Loss 0.6931471824645996 Running Loss 0.6927221172263426\n",
            "Epoch 1 Batch 3421 Loss 0.693149745464325 Running Loss 0.6927233621830899\n",
            "Epoch 1 Batch 3431 Loss 0.6931474208831787 Running Loss 0.6927246030792485\n",
            "Epoch 1 Batch 3441 Loss 0.6931483745574951 Running Loss 0.6927258348922264\n",
            "Epoch 1 Batch 3451 Loss 0.6931495070457458 Running Loss 0.6927270608962498\n",
            "Epoch 1 Batch 3461 Loss 0.6931502819061279 Running Loss 0.6927282795228217\n",
            "Epoch 1 Batch 3471 Loss 0.6931472420692444 Running Loss 0.6927294887406996\n",
            "Epoch 1 Batch 3481 Loss 0.6931473016738892 Running Loss 0.6927306902233947\n",
            "Epoch 1 Batch 3491 Loss 0.6931474208831787 Running Loss 0.6927318861716049\n",
            "Epoch 1 Batch 3501 Loss 0.6931474208831787 Running Loss 0.6927330748281013\n",
            "Epoch 1 Batch 3511 Loss 0.6931471824645996 Running Loss 0.692734259701427\n",
            "Epoch 1 Batch 3521 Loss 0.6931477785110474 Running Loss 0.692735435576034\n",
            "Epoch 1 Batch 3531 Loss 0.6931483745574951 Running Loss 0.6927366046384251\n",
            "Epoch 1 Batch 3541 Loss 0.6931472420692444 Running Loss 0.692737765515534\n",
            "Epoch 1 Batch 3551 Loss 0.693147599697113 Running Loss 0.6927389231274679\n",
            "Epoch 1 Batch 3561 Loss 0.693147599697113 Running Loss 0.6927400725807108\n",
            "Epoch 1 Batch 3571 Loss 0.6931471824645996 Running Loss 0.6927412146281475\n",
            "Epoch 1 Batch 3581 Loss 0.6931575536727905 Running Loss 0.6927423520449053\n",
            "Epoch 1 Batch 3591 Loss 0.6931485533714294 Running Loss 0.6927434823301236\n",
            "Epoch 1 Batch 3601 Loss 0.6931473016738892 Running Loss 0.6927446049307824\n",
            "Epoch 1 Batch 3611 Loss 0.6931473016738892 Running Loss 0.6927457218089614\n",
            "Epoch 1 Batch 3621 Loss 0.6931474208831787 Running Loss 0.6927468341643295\n",
            "Epoch 1 Batch 3631 Loss 0.6931474804878235 Running Loss 0.6927479376677342\n",
            "Epoch 1 Batch 3641 Loss 0.6931498050689697 Running Loss 0.6927490392676817\n",
            "Epoch 1 Batch 3651 Loss 0.6931477785110474 Running Loss 0.6927501315353538\n",
            "Epoch 1 Batch 3661 Loss 0.6931478977203369 Running Loss 0.6927512227691155\n",
            "Epoch 1 Batch 3671 Loss 0.6931471824645996 Running Loss 0.6927523022287713\n",
            "Epoch 1 Batch 3681 Loss 0.6931473016738892 Running Loss 0.692753377296911\n",
            "Epoch 1 Batch 3691 Loss 0.6931477785110474 Running Loss 0.692754447169499\n",
            "Epoch 1 Batch 3701 Loss 0.6931506991386414 Running Loss 0.6927555119047549\n",
            "Epoch 1 Batch 3711 Loss 0.6931484937667847 Running Loss 0.6927565710462993\n",
            "Epoch 1 Batch 3721 Loss 0.6931473612785339 Running Loss 0.6927576228932198\n",
            "Epoch 1 Batch 3731 Loss 0.6931486129760742 Running Loss 0.6927586692934284\n",
            "Epoch 1 Batch 3741 Loss 0.6931706666946411 Running Loss 0.6927597175240995\n",
            "Epoch 1 Batch 3751 Loss 0.6931471824645996 Running Loss 0.6927607520457364\n",
            "Epoch 1 Batch 3761 Loss 0.6931492686271667 Running Loss 0.6927617823656047\n",
            "Epoch 1 Batch 3771 Loss 0.693151593208313 Running Loss 0.6927628095761372\n",
            "Epoch 1 Batch 3781 Loss 0.6931480765342712 Running Loss 0.6927638319364082\n",
            "Epoch 1 Batch 3791 Loss 0.6931472420692444 Running Loss 0.6927648443592095\n",
            "Epoch 1 Batch 3801 Loss 0.6931474804878235 Running Loss 0.6927658516116846\n",
            "Epoch 1 Batch 3811 Loss 0.6931480169296265 Running Loss 0.69276685332789\n",
            "Epoch 1 Batch 3821 Loss 0.6931471824645996 Running Loss 0.6927678494576976\n",
            "Epoch 1 Batch 3831 Loss 0.693151593208313 Running Loss 0.6927688430632042\n",
            "Epoch 1 Batch 3841 Loss 0.6931476593017578 Running Loss 0.692769829400099\n",
            "Epoch 1 Batch 3851 Loss 0.6931471824645996 Running Loss 0.6927708121158339\n",
            "Epoch 1 Batch 3861 Loss 0.693147599697113 Running Loss 0.6927717881664591\n",
            "Epoch 1 Batch 3871 Loss 0.6931474208831787 Running Loss 0.6927727609603358\n",
            "Epoch 1 Batch 3881 Loss 0.6931475400924683 Running Loss 0.6927737271131482\n",
            "Epoch 1 Batch 3891 Loss 0.6931507587432861 Running Loss 0.692774688927933\n",
            "Epoch 1 Batch 3901 Loss 0.6931472420692444 Running Loss 0.6927756455976882\n",
            "Epoch 1 Batch 3911 Loss 0.693147599697113 Running Loss 0.6927765970704379\n",
            "Epoch 1 Batch 3921 Loss 0.6931475400924683 Running Loss 0.6927775434467507\n",
            "Epoch 1 Batch 3931 Loss 0.6931474208831787 Running Loss 0.692778485023287\n",
            "Epoch 1 Batch 3941 Loss 0.6931472420692444 Running Loss 0.6927794226532931\n",
            "Epoch 1 Batch 3951 Loss 0.6931528449058533 Running Loss 0.6927803565779387\n",
            "Epoch 1 Batch 3961 Loss 0.6931474804878235 Running Loss 0.6927812838759033\n",
            "Epoch 1 Batch 3971 Loss 0.6931471824645996 Running Loss 0.6927822077343369\n",
            "Epoch 1 Batch 3981 Loss 0.6931473016738892 Running Loss 0.6927831260231505\n",
            "Epoch 1 Batch 3991 Loss 0.6931473016738892 Running Loss 0.6927840394413403\n",
            "Epoch 1 Batch 4001 Loss 0.6931479573249817 Running Loss 0.6927849491576319\n",
            "Epoch 1 Batch 4011 Loss 0.6931476593017578 Running Loss 0.6927858549173679\n",
            "Epoch 1 Batch 4021 Loss 0.69315105676651 Running Loss 0.6927867563053672\n",
            "Epoch 1 Batch 4031 Loss 0.6931471824645996 Running Loss 0.6927876520381616\n",
            "Epoch 1 Batch 4041 Loss 0.693147599697113 Running Loss 0.6927885428214834\n",
            "Epoch 1 Batch 4051 Loss 0.693153440952301 Running Loss 0.6927894320466789\n",
            "Epoch 1 Batch 4061 Loss 0.6931474208831787 Running Loss 0.6927903137515843\n",
            "Epoch 1 Batch 4071 Loss 0.6931482553482056 Running Loss 0.6927911913591122\n",
            "Epoch 1 Batch 4081 Loss 0.6931487917900085 Running Loss 0.6927920656880746\n",
            "Epoch 1 Batch 4091 Loss 0.6931504607200623 Running Loss 0.6927929360923077\n",
            "Epoch 1 Batch 4101 Loss 0.6931471824645996 Running Loss 0.692793804606238\n",
            "Epoch 1 Batch 4111 Loss 0.6931471824645996 Running Loss 0.6927946647916853\n",
            "Epoch 1 Batch 4121 Loss 0.6931477785110474 Running Loss 0.6927955225236611\n",
            "Epoch 1 Batch 4131 Loss 0.6931472420692444 Running Loss 0.6927963746601142\n",
            "Epoch 1 Batch 4141 Loss 0.6931476593017578 Running Loss 0.6927972267112188\n",
            "Epoch 1 Batch 4151 Loss 0.6931472420692444 Running Loss 0.6927980711964979\n",
            "Epoch 1 Batch 4161 Loss 0.6931471824645996 Running Loss 0.6927989154473946\n",
            "Epoch 1 Batch 4171 Loss 0.693148672580719 Running Loss 0.6927997526205635\n",
            "Epoch 1 Batch 4181 Loss 0.6931471824645996 Running Loss 0.6928005856750286\n",
            "Epoch 1 Batch 4191 Loss 0.6931472420692444 Running Loss 0.6928014131754003\n",
            "Epoch 1 Batch 4201 Loss 0.693148136138916 Running Loss 0.6928022379706059\n",
            "Epoch 1 Batch 4211 Loss 0.6931472420692444 Running Loss 0.6928030586644663\n",
            "Epoch 1 Batch 4221 Loss 0.6931471824645996 Running Loss 0.6928038750319536\n",
            "Epoch 1 Batch 4231 Loss 0.6931474804878235 Running Loss 0.6928046876954232\n",
            "Epoch 1 Batch 4241 Loss 0.6931480169296265 Running Loss 0.6928054986767993\n",
            "Epoch 1 Batch 4251 Loss 0.6931471824645996 Running Loss 0.6928063039357901\n",
            "Epoch 1 Batch 4261 Loss 0.6931471824645996 Running Loss 0.6928071047856309\n",
            "Epoch 1 Batch 4271 Loss 0.6931471824645996 Running Loss 0.6928079020388092\n",
            "Epoch 1 Batch 4281 Loss 0.69315105676651 Running Loss 0.6928086979899889\n",
            "Epoch 1 Batch 4291 Loss 0.6931478381156921 Running Loss 0.6928094882588375\n",
            "Epoch 1 Batch 4301 Loss 0.6931475400924683 Running Loss 0.6928102753517711\n",
            "Epoch 1 Batch 4311 Loss 0.6931471824645996 Running Loss 0.6928110586687131\n",
            "Epoch 1 Batch 4321 Loss 0.6931472420692444 Running Loss 0.6928118377944656\n",
            "Epoch 1 Batch 4331 Loss 0.6931477785110474 Running Loss 0.6928126139829077\n",
            "Epoch 1 Batch 4341 Loss 0.6931507587432861 Running Loss 0.6928133858950065\n",
            "Epoch 1 Batch 4351 Loss 0.6931480169296265 Running Loss 0.6928141546424738\n",
            "Epoch 1 Batch 4361 Loss 0.6931477785110474 Running Loss 0.6928149199737264\n",
            "Epoch 1 Batch 4371 Loss 0.6931473612785339 Running Loss 0.6928156808349367\n",
            "Epoch 1 Batch 4381 Loss 0.6931477785110474 Running Loss 0.6928164382771089\n",
            "Epoch 1 Batch 4391 Loss 0.6931471824645996 Running Loss 0.6928171922557312\n",
            "Epoch 1 Batch 4401 Loss 0.6931483745574951 Running Loss 0.6928179460583803\n",
            "Epoch 1 Batch 4411 Loss 0.6931484937667847 Running Loss 0.6928186931460927\n",
            "Epoch 1 Batch 4421 Loss 0.6931474208831787 Running Loss 0.692819438728103\n",
            "Epoch 1 Batch 4431 Loss 0.6931479573249817 Running Loss 0.6928201795592853\n",
            "Epoch 1 Batch 4441 Loss 0.6931480169296265 Running Loss 0.6928209172151991\n",
            "Epoch 1 Batch 4451 Loss 0.6931480765342712 Running Loss 0.6928216515297676\n",
            "Epoch 1 Batch 4461 Loss 0.6931477785110474 Running Loss 0.6928223823116809\n",
            "Epoch 1 Batch 4471 Loss 0.693147599697113 Running Loss 0.6928231115443572\n",
            "Epoch 1 Batch 4481 Loss 0.6931475400924683 Running Loss 0.6928238359659651\n",
            "Epoch 1 Batch 4491 Loss 0.6931471824645996 Running Loss 0.6928245564713241\n",
            "Epoch 1 Batch 4501 Loss 0.6931480765342712 Running Loss 0.6928252750861594\n",
            "Epoch 1 Batch 4511 Loss 0.693149745464325 Running Loss 0.6928259909509741\n",
            "Epoch 1 Batch 4521 Loss 0.6931473612785339 Running Loss 0.6928267035302903\n",
            "Epoch 1 Batch 4531 Loss 0.6931484937667847 Running Loss 0.6928274131221136\n",
            "Epoch 1 Batch 4541 Loss 0.6931475400924683 Running Loss 0.6928281179610603\n",
            "Epoch 1 Batch 4551 Loss 0.6931471824645996 Running Loss 0.692828820737161\n",
            "Epoch 1 Batch 4561 Loss 0.6931498050689697 Running Loss 0.6928295205753379\n",
            "Epoch 1 Batch 4571 Loss 0.6931475400924683 Running Loss 0.6928302175209523\n",
            "Epoch 1 Batch 4581 Loss 0.6931476593017578 Running Loss 0.6928309106691465\n",
            "Epoch 1 Batch 4591 Loss 0.6931478977203369 Running Loss 0.692831600979506\n",
            "Epoch 1 Batch 4601 Loss 0.6931471824645996 Running Loss 0.6928322876673423\n",
            "Epoch 1 Batch 4611 Loss 0.6931471824645996 Running Loss 0.6928329717903534\n",
            "Epoch 1 Batch 4621 Loss 0.6931519508361816 Running Loss 0.6928336551838998\n",
            "Epoch 1 Batch 4631 Loss 0.6931473016738892 Running Loss 0.6928343348151997\n",
            "Epoch 1 Batch 4641 Loss 0.6931473016738892 Running Loss 0.692835010336124\n",
            "Epoch 1 Batch 4651 Loss 0.6931475400924683 Running Loss 0.6928356844003521\n",
            "Epoch 1 Batch 4661 Loss 0.6931474208831787 Running Loss 0.69283635489446\n",
            "Epoch 1 Batch 4671 Loss 0.6931473016738892 Running Loss 0.6928370241510411\n",
            "Epoch 1 Batch 4681 Loss 0.6931495666503906 Running Loss 0.69283768984783\n",
            "Epoch 1 Batch 4691 Loss 0.6931473612785339 Running Loss 0.6928383514993457\n",
            "Epoch 1 Batch 4701 Loss 0.6931473016738892 Running Loss 0.6928390124913759\n",
            "Epoch 1 Batch 4711 Loss 0.693148136138916 Running Loss 0.692839668576972\n",
            "Epoch 1 Batch 4721 Loss 0.6931472420692444 Running Loss 0.6928403206205903\n",
            "Epoch 1 Batch 4731 Loss 0.6931474208831787 Running Loss 0.6928409708274441\n",
            "Epoch 1 Batch 4741 Loss 0.693149209022522 Running Loss 0.6928416197120427\n",
            "Epoch 1 Batch 4751 Loss 0.6931477785110474 Running Loss 0.692842264146309\n",
            "Epoch 1 Batch 4761 Loss 0.6931527256965637 Running Loss 0.6928429082395949\n",
            "Epoch 1 Batch 4771 Loss 0.6931472420692444 Running Loss 0.6928435491456147\n",
            "Epoch 1 Batch 4781 Loss 0.6931473016738892 Running Loss 0.692844185463132\n",
            "Epoch 1 Batch 4791 Loss 0.6931474208831787 Running Loss 0.6928448188879673\n",
            "Epoch 1 Batch 4801 Loss 0.6931473612785339 Running Loss 0.6928454501334391\n",
            "Epoch 1 Batch 4811 Loss 0.6931474804878235 Running Loss 0.692846078903406\n",
            "Epoch 1 Batch 4821 Loss 0.693147599697113 Running Loss 0.6928467047063673\n",
            "Epoch 1 Batch 4831 Loss 0.6931471824645996 Running Loss 0.692847327770493\n",
            "Epoch 1 Batch 4841 Loss 0.6931471824645996 Running Loss 0.692847947989631\n",
            "Epoch 1 Batch 4851 Loss 0.6931477785110474 Running Loss 0.6928485663889168\n",
            "Epoch 1 Batch 4861 Loss 0.6931473612785339 Running Loss 0.6928491822561348\n",
            "Epoch 1 Batch 4871 Loss 0.6931499242782593 Running Loss 0.6928497965980471\n",
            "Epoch 1 Batch 4881 Loss 0.6931483745574951 Running Loss 0.6928504085325848\n",
            "Epoch 1 Batch 4891 Loss 0.6931474804878235 Running Loss 0.6928510170020933\n",
            "Epoch 1 Batch 4901 Loss 0.6931475400924683 Running Loss 0.6928516225142519\n",
            "Epoch 1 Batch 4911 Loss 0.6931473612785339 Running Loss 0.6928522254512354\n",
            "Epoch 1 Batch 4921 Loss 0.6931472420692444 Running Loss 0.6928528275608024\n",
            "Epoch 1 Batch 4931 Loss 0.6931500434875488 Running Loss 0.6928534267930708\n",
            "Epoch 1 Batch 4941 Loss 0.6931471824645996 Running Loss 0.692854021729981\n",
            "Epoch 1 Batch 4951 Loss 0.6931496858596802 Running Loss 0.6928546197172175\n",
            "Epoch 1 Batch 4961 Loss 0.6931495070457458 Running Loss 0.6928552118695279\n",
            "Epoch 1 Batch 4971 Loss 0.6931477785110474 Running Loss 0.6928558016034396\n",
            "Epoch 1 Batch 4981 Loss 0.6931501626968384 Running Loss 0.6928563878565422\n",
            "Epoch 1 Batch 4991 Loss 0.6931471824645996 Running Loss 0.6928569712349371\n",
            "Epoch 1 Batch 5001 Loss 0.693147599697113 Running Loss 0.6928575524471445\n",
            "Epoch 1 Batch 5011 Loss 0.6931471824645996 Running Loss 0.6928581326718198\n",
            "Epoch 1 Batch 5021 Loss 0.6931476593017578 Running Loss 0.6928587092201303\n",
            "Epoch 1 Batch 5031 Loss 0.6931474208831787 Running Loss 0.6928592849692397\n",
            "Epoch 1 Batch 5041 Loss 0.693149209022522 Running Loss 0.692859858221252\n",
            "Epoch 1 Batch 5051 Loss 0.6931473612785339 Running Loss 0.6928604281885604\n",
            "Epoch 1 Batch 5061 Loss 0.6931474208831787 Running Loss 0.692860996586559\n",
            "Epoch 1 Batch 5071 Loss 0.6931472420692444 Running Loss 0.6928615621668516\n",
            "Epoch 1 Batch 5081 Loss 0.6931481957435608 Running Loss 0.6928621257789678\n",
            "Epoch 1 Batch 5091 Loss 0.6931473016738892 Running Loss 0.6928626880433137\n",
            "Epoch 1 Batch 5101 Loss 0.6931478381156921 Running Loss 0.692863247740902\n",
            "Epoch 1 Batch 5111 Loss 0.6931474208831787 Running Loss 0.6928638041404287\n",
            "Epoch 1 Batch 5121 Loss 0.6931471824645996 Running Loss 0.6928643586113685\n",
            "Epoch 1 Batch 5131 Loss 0.6931478381156921 Running Loss 0.6928649104215369\n",
            "Epoch 1 Batch 5141 Loss 0.6931471824645996 Running Loss 0.6928654601081896\n",
            "Epoch 1 Batch 5151 Loss 0.6931471824645996 Running Loss 0.69286600812341\n",
            "Epoch 1 Batch 5161 Loss 0.6931472420692444 Running Loss 0.692866553737775\n",
            "Epoch 1 Batch 5171 Loss 0.693148136138916 Running Loss 0.6928670989017011\n",
            "Epoch 1 Batch 5181 Loss 0.6931471824645996 Running Loss 0.6928676410062827\n",
            "Epoch 1 Batch 5191 Loss 0.6931474208831787 Running Loss 0.6928681800232712\n",
            "Epoch 1 Batch 5201 Loss 0.6931475400924683 Running Loss 0.692868717517607\n",
            "Epoch 1 Batch 5211 Loss 0.6931479573249817 Running Loss 0.6928692523885474\n",
            "Epoch 1 Batch 5221 Loss 0.6931482553482056 Running Loss 0.6928697852790641\n",
            "Epoch 1 Batch 5231 Loss 0.6931471824645996 Running Loss 0.6928703168955799\n",
            "Epoch 1 Batch 5241 Loss 0.6931471824645996 Running Loss 0.692870846108111\n",
            "Epoch 1 Batch 5251 Loss 0.6931477785110474 Running Loss 0.6928713732255206\n",
            "Epoch 1 Batch 5261 Loss 0.6931472420692444 Running Loss 0.6928718979765176\n",
            "Epoch 1 Batch 5271 Loss 0.6931477785110474 Running Loss 0.6928724213922935\n",
            "Epoch 1 Batch 5281 Loss 0.6931478381156921 Running Loss 0.6928729422276184\n",
            "Epoch 1 Batch 5291 Loss 0.6931471824645996 Running Loss 0.6928734613532854\n",
            "Epoch 1 Batch 5301 Loss 0.6931472420692444 Running Loss 0.6928739786890178\n",
            "Epoch 1 Batch 5311 Loss 0.6931472420692444 Running Loss 0.6928744960854764\n",
            "Epoch 1 Batch 5321 Loss 0.693148136138916 Running Loss 0.692875010136979\n",
            "Epoch 1 Batch 5331 Loss 0.6931486129760742 Running Loss 0.6928755225171025\n",
            "Epoch 1 Batch 5341 Loss 0.6931482553482056 Running Loss 0.6928760318737355\n",
            "Epoch 1 Batch 5351 Loss 0.6931471824645996 Running Loss 0.692876539482533\n",
            "Epoch 1 Batch 5361 Loss 0.6931477189064026 Running Loss 0.6928770465318046\n",
            "Epoch 1 Batch 5371 Loss 0.6931485533714294 Running Loss 0.6928775508495665\n",
            "Epoch 1 Batch 5381 Loss 0.6931471824645996 Running Loss 0.6928780533261203\n",
            "Epoch 1 Batch 5391 Loss 0.6931476593017578 Running Loss 0.6928785541265003\n",
            "Epoch 1 Batch 5401 Loss 0.6931480765342712 Running Loss 0.6928790521785034\n",
            "Epoch 1 Batch 5411 Loss 0.6931481957435608 Running Loss 0.6928795485328205\n",
            "Epoch 1 Batch 5421 Loss 0.6931478977203369 Running Loss 0.6928800432208366\n",
            "Epoch 1 Batch 5431 Loss 0.6931478977203369 Running Loss 0.6928805357359364\n",
            "Epoch 1 Batch 5441 Loss 0.6931483745574951 Running Loss 0.6928810280509959\n",
            "Epoch 1 Batch 5451 Loss 0.6931477785110474 Running Loss 0.6928815166571013\n",
            "Epoch 1 Batch 5461 Loss 0.6931473016738892 Running Loss 0.6928820051000442\n",
            "Epoch 1 Batch 5471 Loss 0.6931474208831787 Running Loss 0.692882490755108\n",
            "Epoch 1 Batch 5481 Loss 0.6931471824645996 Running Loss 0.692882974616282\n",
            "Epoch 1 Batch 5491 Loss 0.6931478977203369 Running Loss 0.6928834569538862\n",
            "Epoch 1 Batch 5501 Loss 0.6931490898132324 Running Loss 0.6928839373644905\n",
            "Epoch 1 Batch 5511 Loss 0.6931499242782593 Running Loss 0.6928844174592903\n",
            "Epoch 1 Batch 5521 Loss 0.6931473016738892 Running Loss 0.6928848944006569\n",
            "Epoch 1 Batch 5531 Loss 0.6931475400924683 Running Loss 0.6928853689492708\n",
            "Epoch 1 Batch 5541 Loss 0.6931519508361816 Running Loss 0.6928858435491732\n",
            "Epoch 1 Batch 5551 Loss 0.693148136138916 Running Loss 0.6928863152364979\n",
            "Epoch 1 Batch 5561 Loss 0.6931471824645996 Running Loss 0.6928867848415509\n",
            "Epoch 1 Batch 5571 Loss 0.6931473016738892 Running Loss 0.6928872528891015\n",
            "Epoch 1 Batch 5581 Loss 0.6931472420692444 Running Loss 0.6928877208613512\n",
            "Epoch 1 Batch 5591 Loss 0.6931485533714294 Running Loss 0.6928881867224874\n",
            "Epoch 1 Batch 5601 Loss 0.6931480765342712 Running Loss 0.6928886502922657\n",
            "Epoch 1 Batch 5611 Loss 0.6931482553482056 Running Loss 0.6928891130170175\n",
            "Epoch 1 Batch 5621 Loss 0.6931487917900085 Running Loss 0.6928895732576451\n",
            "Epoch 1 Batch 5631 Loss 0.6931474208831787 Running Loss 0.6928900311967446\n",
            "Epoch 1 Batch 5641 Loss 0.6931474208831787 Running Loss 0.6928904896254997\n",
            "Epoch 1 Batch 5651 Loss 0.6931486129760742 Running Loss 0.6928909449129268\n",
            "Epoch 1 Batch 5661 Loss 0.6931477785110474 Running Loss 0.6928913991498854\n",
            "Epoch 1 Batch 5671 Loss 0.6931471824645996 Running Loss 0.6928918520476406\n",
            "Epoch 1 Batch 5681 Loss 0.6931473016738892 Running Loss 0.6928923022178377\n",
            "Epoch 1 Batch 5691 Loss 0.6931498646736145 Running Loss 0.6928927511620911\n",
            "Epoch 1 Batch 5701 Loss 0.6931473612785339 Running Loss 0.6928931997232616\n",
            "Epoch 1 Batch 5711 Loss 0.6931478381156921 Running Loss 0.6928936459308037\n",
            "Epoch 1 Batch 5721 Loss 0.6931477189064026 Running Loss 0.6928940906618009\n",
            "Epoch 1 Batch 5731 Loss 0.6931478977203369 Running Loss 0.6928945329151444\n",
            "Epoch 1 Batch 5741 Loss 0.6931471824645996 Running Loss 0.6928949747802363\n",
            "Epoch 1 Batch 5751 Loss 0.6931474208831787 Running Loss 0.6928954139271523\n",
            "Epoch 1 Batch 5761 Loss 0.6931486129760742 Running Loss 0.6928958522840996\n",
            "Epoch 1 Batch 5771 Loss 0.6931471824645996 Running Loss 0.6928962887293998\n",
            "Epoch 1 Batch 5781 Loss 0.6931471824645996 Running Loss 0.6928967236544594\n",
            "Epoch 1 Batch 5791 Loss 0.6931473016738892 Running Loss 0.692897157098032\n",
            "Epoch 1 Batch 5801 Loss 0.693147599697113 Running Loss 0.6928975894582249\n",
            "Epoch 1 Batch 5811 Loss 0.6931471824645996 Running Loss 0.692898019366165\n",
            "Epoch 1 Batch 5821 Loss 0.6931471824645996 Running Loss 0.6928984490052835\n",
            "Epoch 1 Batch 5831 Loss 0.6931475400924683 Running Loss 0.6928988772320964\n",
            "Epoch 1 Batch 5841 Loss 0.6931474208831787 Running Loss 0.6928993031864724\n",
            "Epoch 1 Batch 5851 Loss 0.6931473016738892 Running Loss 0.6928997278885849\n",
            "Epoch 1 Batch 5861 Loss 0.6931495666503906 Running Loss 0.6929001514465405\n",
            "Epoch 1 Batch 5871 Loss 0.6931481957435608 Running Loss 0.6929005730946046\n",
            "Epoch 1 Batch 5881 Loss 0.6931491494178772 Running Loss 0.692900994099275\n",
            "Epoch 1 Batch 5891 Loss 0.693147599697113 Running Loss 0.6929014129663764\n",
            "Epoch 1 Batch 5901 Loss 0.6931471824645996 Running Loss 0.6929018303027216\n",
            "Epoch 1 Batch 5911 Loss 0.6931472420692444 Running Loss 0.6929022462875019\n",
            "Epoch 1 Batch 5921 Loss 0.693147599697113 Running Loss 0.692902660655766\n",
            "Epoch 1 Batch 5931 Loss 0.6931471824645996 Running Loss 0.6929030736669323\n",
            "Epoch 1 Batch 5941 Loss 0.6931473016738892 Running Loss 0.6929034851271985\n",
            "Epoch 1 Batch 5951 Loss 0.6931493878364563 Running Loss 0.6929038958156075\n",
            "Epoch 1 Batch 5961 Loss 0.6931471824645996 Running Loss 0.6929043047161355\n",
            "Epoch 1 Batch 5971 Loss 0.6931473612785339 Running Loss 0.6929047120074654\n",
            "Epoch 1 Batch 5981 Loss 0.6931511163711548 Running Loss 0.6929051185746473\n",
            "Epoch 1 Batch 5991 Loss 0.6931473016738892 Running Loss 0.692905523068239\n",
            "Epoch 1 Batch 6001 Loss 0.6931471824645996 Running Loss 0.6929059271374617\n",
            "Epoch 1 Batch 6011 Loss 0.6931474804878235 Running Loss 0.6929063289301544\n",
            "Epoch 1 Batch 6021 Loss 0.6931496858596802 Running Loss 0.6929067301504677\n",
            "Epoch 1 Batch 6031 Loss 0.6931473016738892 Running Loss 0.6929071294670377\n",
            "Epoch 1 Batch 6041 Loss 0.6931471824645996 Running Loss 0.6929075276786534\n",
            "Epoch 1 Batch 6051 Loss 0.6931471824645996 Running Loss 0.6929079246922891\n",
            "Epoch 1 Batch 6061 Loss 0.6931474208831787 Running Loss 0.6929083201893484\n",
            "Epoch 1 Batch 6071 Loss 0.6931472420692444 Running Loss 0.6929087159150987\n",
            "Epoch 1 Batch 6081 Loss 0.6931472420692444 Running Loss 0.6929091086730305\n",
            "Epoch 1 Batch 6091 Loss 0.6931471824645996 Running Loss 0.692909500444685\n",
            "Epoch 1 Batch 6101 Loss 0.6931471824645996 Running Loss 0.692909891625698\n",
            "Epoch 1 Batch 6111 Loss 0.6931481957435608 Running Loss 0.6929102863252621\n",
            "Epoch 1 Batch 6121 Loss 0.693148136138916 Running Loss 0.69291067659962\n",
            "Epoch 1 Batch 6131 Loss 0.6931473016738892 Running Loss 0.6929110633356694\n",
            "Epoch 1 Batch 6141 Loss 0.6931475400924683 Running Loss 0.6929114484336628\n",
            "Epoch 1 Batch 6151 Loss 0.6931471824645996 Running Loss 0.6929118326961887\n",
            "Epoch 1 Batch 6161 Loss 0.6931480169296265 Running Loss 0.6929122150244215\n",
            "Epoch 1 Batch 6171 Loss 0.6931498646736145 Running Loss 0.6929125964226241\n",
            "Epoch 1 Batch 6181 Loss 0.6931473016738892 Running Loss 0.6929129767506625\n",
            "Epoch 1 Batch 6191 Loss 0.6931483745574951 Running Loss 0.6929133565913801\n",
            "Epoch 1 Batch 6201 Loss 0.6931484341621399 Running Loss 0.6929137352454511\n",
            "Epoch 1 Batch 6211 Loss 0.6931486129760742 Running Loss 0.6929141128049766\n",
            "Epoch 1 Batch 6221 Loss 0.6931472420692444 Running Loss 0.692914487684756\n",
            "Epoch 1 Batch 6231 Loss 0.6931494474411011 Running Loss 0.6929148630161497\n",
            "Epoch 1 Batch 6241 Loss 0.6931471824645996 Running Loss 0.6929152358076813\n",
            "Epoch 1 Batch 6251 Loss 0.693148136138916 Running Loss 0.6929156081025418\n",
            "Epoch 1 Batch 6261 Loss 0.6931471824645996 Running Loss 0.6929159798364711\n",
            "Epoch 1 Batch 6271 Loss 0.6931473612785339 Running Loss 0.6929163489020853\n",
            "Epoch 1 Batch 6281 Loss 0.6931474804878235 Running Loss 0.6929167171246572\n",
            "Epoch 1 Batch 6291 Loss 0.6931471824645996 Running Loss 0.6929170851524789\n",
            "Epoch 1 Batch 6301 Loss 0.6931471824645996 Running Loss 0.6929174510661883\n",
            "Epoch 1 Batch 6311 Loss 0.6931477785110474 Running Loss 0.6929178156502895\n",
            "Epoch 1 Batch 6321 Loss 0.6931473016738892 Running Loss 0.6929181791939816\n",
            "Epoch 1 Batch 6331 Loss 0.6931499242782593 Running Loss 0.692918542380055\n",
            "Epoch 1 Batch 6341 Loss 0.6931473016738892 Running Loss 0.6929189034806237\n",
            "Epoch 1 Batch 6351 Loss 0.6931471824645996 Running Loss 0.6929192634534318\n",
            "Epoch 1 Batch 6361 Loss 0.6931472420692444 Running Loss 0.6929196225099454\n",
            "Epoch 1 Batch 6371 Loss 0.6931473016738892 Running Loss 0.6929199802615432\n",
            "Epoch 1 Batch 6381 Loss 0.6931475400924683 Running Loss 0.6929203366769965\n",
            "Epoch 1 Batch 6391 Loss 0.6931474208831787 Running Loss 0.692920692415421\n",
            "Epoch 1 Batch 6401 Loss 0.6931473016738892 Running Loss 0.6929210476103546\n",
            "Epoch 1 Batch 6411 Loss 0.6931475400924683 Running Loss 0.6929214021806653\n",
            "Epoch 1 Batch 6421 Loss 0.6931471824645996 Running Loss 0.6929217541706078\n",
            "Epoch 1 Batch 6431 Loss 0.6931471824645996 Running Loss 0.6929221056590572\n",
            "Epoch 1 Batch 6441 Loss 0.6931478977203369 Running Loss 0.6929224559265419\n",
            "Epoch 1 Batch 6451 Loss 0.6931472420692444 Running Loss 0.6929228050064586\n",
            "Epoch 1 Batch 6461 Loss 0.6931475400924683 Running Loss 0.6929231535500924\n",
            "Epoch 1 Batch 6471 Loss 0.6931472420692444 Running Loss 0.6929235001690622\n",
            "Epoch 1 Batch 6481 Loss 0.6931478381156921 Running Loss 0.6929238464265414\n",
            "Epoch 1 Batch 6491 Loss 0.6931471824645996 Running Loss 0.6929241907998789\n",
            "Epoch 1 Batch 6501 Loss 0.6931480169296265 Running Loss 0.6929245350122856\n",
            "Epoch 1 Batch 6511 Loss 0.6931471824645996 Running Loss 0.692924878927186\n",
            "Epoch 1 Batch 6521 Loss 0.6931474804878235 Running Loss 0.6929252204802153\n",
            "Epoch 1 Batch 6531 Loss 0.6931485533714294 Running Loss 0.692925561042059\n",
            "Epoch 1 Batch 6541 Loss 0.6931507587432861 Running Loss 0.6929259011913487\n",
            "Epoch 1 Batch 6551 Loss 0.6931473016738892 Running Loss 0.6929262394469088\n",
            "Epoch 1 Batch 6561 Loss 0.6931471824645996 Running Loss 0.6929265776525054\n",
            "Epoch 1 Batch 6571 Loss 0.6931473016738892 Running Loss 0.6929269163798314\n",
            "Epoch 1 Batch 6581 Loss 0.6931474208831787 Running Loss 0.6929272513515662\n",
            "Epoch 1 Batch 6591 Loss 0.6931482553482056 Running Loss 0.6929275857137993\n",
            "Epoch 1 Batch 6601 Loss 0.6931475400924683 Running Loss 0.6929279189184929\n",
            "Epoch 1 Batch 6611 Loss 0.6931472420692444 Running Loss 0.6929282508266439\n",
            "Epoch 1 Batch 6621 Loss 0.6931476593017578 Running Loss 0.6929285818942452\n",
            "Epoch 1 Batch 6631 Loss 0.6931512951850891 Running Loss 0.6929289332667261\n",
            "Epoch 1 Batch 6641 Loss 0.6931474208831787 Running Loss 0.6929292662587803\n",
            "Epoch 1 Batch 6651 Loss 0.6931504011154175 Running Loss 0.6929295969590121\n",
            "Epoch 1 Batch 6661 Loss 0.6931474804878235 Running Loss 0.6929299256103996\n",
            "Epoch 1 Batch 6671 Loss 0.693147599697113 Running Loss 0.6929302521238726\n",
            "Epoch 1 Batch 6681 Loss 0.6931482553482056 Running Loss 0.6929305786591164\n",
            "Epoch 1 Batch 6691 Loss 0.6931509375572205 Running Loss 0.6929309055545444\n",
            "Epoch 1 Batch 6701 Loss 0.693148136138916 Running Loss 0.6929312320257938\n",
            "Epoch 1 Batch 6711 Loss 0.6931496858596802 Running Loss 0.6929315569645565\n",
            "Epoch 1 Batch 6721 Loss 0.6931475400924683 Running Loss 0.6929318797923575\n",
            "Epoch 1 Batch 6731 Loss 0.6931473016738892 Running Loss 0.692932200297224\n",
            "Epoch 1 Batch 6741 Loss 0.6931473612785339 Running Loss 0.6929325200722314\n",
            "Epoch 1 Batch 6751 Loss 0.6931474804878235 Running Loss 0.6929328389881876\n",
            "Epoch 1 Batch 6761 Loss 0.6931474208831787 Running Loss 0.6929331563083647\n",
            "Epoch 1 Batch 6771 Loss 0.6931473612785339 Running Loss 0.6929334732634403\n",
            "Epoch 1 Batch 6781 Loss 0.6931480169296265 Running Loss 0.6929337900044591\n",
            "Epoch 1 Batch 6791 Loss 0.6931473612785339 Running Loss 0.6929341051982615\n",
            "Epoch 1 Batch 6801 Loss 0.6931476593017578 Running Loss 0.6929344197894314\n",
            "Epoch 1 Batch 6811 Loss 0.6931482553482056 Running Loss 0.692934733176788\n",
            "Epoch 1 Batch 6821 Loss 0.6931488513946533 Running Loss 0.6929350452869802\n",
            "Epoch 1 Batch 6831 Loss 0.6931483745574951 Running Loss 0.6929353568149406\n",
            "Epoch 1 Batch 6841 Loss 0.6931473612785339 Running Loss 0.6929356671707478\n",
            "Epoch 1 Batch 6851 Loss 0.6931483745574951 Running Loss 0.6929359775949544\n",
            "Epoch 1 Batch 6861 Loss 0.6931471824645996 Running Loss 0.6929362863063325\n",
            "Epoch 1 Batch 6871 Loss 0.6931473016738892 Running Loss 0.6929365941711674\n",
            "Epoch 1 Batch 6881 Loss 0.6931495666503906 Running Loss 0.6929369013577314\n",
            "Epoch 1 Batch 6891 Loss 0.6931472420692444 Running Loss 0.692937206839671\n",
            "Epoch 1 Batch 6901 Loss 0.6931474804878235 Running Loss 0.6929375114535581\n",
            "Epoch 1 Batch 6911 Loss 0.6931472420692444 Running Loss 0.6929378162467381\n",
            "Epoch 1 Batch 6921 Loss 0.6931474804878235 Running Loss 0.6929381200471825\n",
            "Epoch 1 Batch 6931 Loss 0.6931475400924683 Running Loss 0.6929384223346052\n",
            "Epoch 1 Batch 6941 Loss 0.6931477189064026 Running Loss 0.6929387235191503\n",
            "Epoch 1 Batch 6951 Loss 0.6931471824645996 Running Loss 0.6929390239657266\n",
            "Epoch 1 Batch 6961 Loss 0.6931472420692444 Running Loss 0.6929393236090133\n",
            "Epoch 1 Batch 6971 Loss 0.6931473016738892 Running Loss 0.6929396224695681\n",
            "Epoch 1 Batch 6981 Loss 0.6931474804878235 Running Loss 0.6929399200299291\n",
            "Epoch 1 Batch 6991 Loss 0.6931473016738892 Running Loss 0.6929402176512957\n",
            "Epoch 1 Batch 7001 Loss 0.6931473016738892 Running Loss 0.6929405138775581\n",
            "Epoch 1 Batch 7011 Loss 0.6931473016738892 Running Loss 0.6929408094968321\n",
            "Epoch 1 Batch 7021 Loss 0.6931484341621399 Running Loss 0.6929411043589008\n",
            "Epoch 1 Batch 7031 Loss 0.6931475400924683 Running Loss 0.6929413982041953\n",
            "Epoch 1 Batch 7041 Loss 0.6931471824645996 Running Loss 0.692941691663485\n",
            "Epoch 1 Batch 7051 Loss 0.6931471824645996 Running Loss 0.6929419834957683\n",
            "Epoch 1 Batch 7061 Loss 0.6931478977203369 Running Loss 0.6929422746787175\n",
            "Epoch 1 Batch 7071 Loss 0.6931471824645996 Running Loss 0.6929425652825231\n",
            "Epoch 1 Batch 7081 Loss 0.6931472420692444 Running Loss 0.6929428546109824\n",
            "Epoch 1 Batch 7091 Loss 0.6931482553482056 Running Loss 0.6929431438294743\n",
            "Epoch 1 Batch 7101 Loss 0.6931487321853638 Running Loss 0.6929434322669563\n",
            "Epoch 1 Batch 7111 Loss 0.6931475400924683 Running Loss 0.6929437197423186\n",
            "Epoch 1 Batch 7121 Loss 0.6931471824645996 Running Loss 0.6929440057071771\n",
            "Epoch 1 Batch 7131 Loss 0.6931471824645996 Running Loss 0.6929442912712118\n",
            "Epoch 1 Batch 7141 Loss 0.6931471824645996 Running Loss 0.6929445761690085\n",
            "Epoch 1 Batch 7151 Loss 0.6931491494178772 Running Loss 0.6929448603533511\n",
            "Epoch 1 Batch 7161 Loss 0.6931477189064026 Running Loss 0.6929451438938168\n",
            "Epoch 1 Batch 7171 Loss 0.6931473016738892 Running Loss 0.6929454261447708\n",
            "Epoch 1 Batch 7181 Loss 0.6931475400924683 Running Loss 0.6929457086388604\n",
            "Epoch 1 Batch 7191 Loss 0.6931471824645996 Running Loss 0.6929459890956557\n",
            "Epoch 1 Batch 7201 Loss 0.6931476593017578 Running Loss 0.6929462693611991\n",
            "Epoch 1 Batch 7211 Loss 0.6931471824645996 Running Loss 0.6929465482873405\n",
            "Epoch 1 Batch 7221 Loss 0.6931473016738892 Running Loss 0.6929468286118338\n",
            "Epoch 1 Batch 7231 Loss 0.6931472420692444 Running Loss 0.6929471066360436\n",
            "Epoch 1 Batch 7241 Loss 0.6931514739990234 Running Loss 0.6929473841886727\n",
            "Epoch 1 Batch 7251 Loss 0.6931474208831787 Running Loss 0.6929476600879637\n",
            "Epoch 1 Batch 7261 Loss 0.6931473016738892 Running Loss 0.6929479353668572\n",
            "Epoch 1 Batch 7271 Loss 0.6931472420692444 Running Loss 0.6929482097082068\n",
            "Epoch 1 Batch 7281 Loss 0.6931473612785339 Running Loss 0.6929484842128437\n",
            "Epoch 1 Batch 7291 Loss 0.6931472420692444 Running Loss 0.6929487574331032\n",
            "Epoch 1 Batch 7301 Loss 0.6931471824645996 Running Loss 0.6929490294559021\n",
            "Epoch 1 Batch 7311 Loss 0.6931500434875488 Running Loss 0.6929493013541621\n",
            "Epoch 1 Batch 7321 Loss 0.6931472420692444 Running Loss 0.6929495720699859\n",
            "Epoch 1 Batch 7331 Loss 0.6931472420692444 Running Loss 0.6929498418683879\n",
            "Epoch 1 Batch 7341 Loss 0.6931477785110474 Running Loss 0.6929501111022525\n",
            "Epoch 1 Batch 7351 Loss 0.6931475400924683 Running Loss 0.6929503795630655\n",
            "Epoch 1 Batch 7361 Loss 0.6931474804878235 Running Loss 0.6929506475211906\n",
            "Epoch 1 Batch 7371 Loss 0.6931494474411011 Running Loss 0.6929509152616967\n",
            "Epoch 1 Batch 7381 Loss 0.6931475400924683 Running Loss 0.692951181808343\n",
            "Epoch 1 Batch 7391 Loss 0.6931474208831787 Running Loss 0.6929514474240389\n",
            "Epoch 1 Batch 7401 Loss 0.6931477785110474 Running Loss 0.6929517123622196\n",
            "Epoch 1 Batch 7411 Loss 0.6931477785110474 Running Loss 0.6929519771966611\n",
            "Epoch 1 Batch 7421 Loss 0.6931473612785339 Running Loss 0.6929522408274141\n",
            "Epoch 1 Batch 7431 Loss 0.6931491494178772 Running Loss 0.6929525037165398\n",
            "Epoch 1 Batch 7441 Loss 0.6931499242782593 Running Loss 0.6929527662274915\n",
            "Epoch 1 Batch 7451 Loss 0.693148136138916 Running Loss 0.6929530276738306\n",
            "Epoch 1 Batch 7461 Loss 0.6931524872779846 Running Loss 0.6929532891942515\n",
            "Epoch 1 Batch 7471 Loss 0.6931477785110474 Running Loss 0.692953549687474\n",
            "Epoch 1 Batch 7481 Loss 0.6931476593017578 Running Loss 0.6929538089663981\n",
            "Epoch 1 Batch 7491 Loss 0.6931472420692444 Running Loss 0.6929540679031818\n",
            "Epoch 1 Batch 7501 Loss 0.6931474208831787 Running Loss 0.6929543261575056\n",
            "Epoch 1 Batch 7511 Loss 0.6931471824645996 Running Loss 0.6929545834384766\n",
            "Epoch 1 Batch 7521 Loss 0.6931493282318115 Running Loss 0.6929548405107864\n",
            "Epoch 1 Batch 7531 Loss 0.6931472420692444 Running Loss 0.6929550963305426\n",
            "Epoch 1 Batch 7541 Loss 0.6931471824645996 Running Loss 0.6929553517168483\n",
            "Epoch 1 Batch 7551 Loss 0.6931493282318115 Running Loss 0.6929556064661916\n",
            "Epoch 1 Batch 7561 Loss 0.6931471824645996 Running Loss 0.69295586040767\n",
            "Epoch 1 Batch 7571 Loss 0.6931473016738892 Running Loss 0.6929561138357768\n",
            "Epoch 1 Batch 7581 Loss 0.6931480765342712 Running Loss 0.692956366445911\n",
            "Epoch 1 Batch 7591 Loss 0.6931475400924683 Running Loss 0.6929566181627851\n",
            "Epoch 1 Batch 7601 Loss 0.6931480765342712 Running Loss 0.692956869578051\n",
            "Epoch 1 Batch 7611 Loss 0.693149983882904 Running Loss 0.6929571202543402\n",
            "Epoch 1 Batch 7621 Loss 0.6931473016738892 Running Loss 0.6929573705543325\n",
            "Epoch 1 Batch 7631 Loss 0.6931474208831787 Running Loss 0.6929576201592621\n",
            "Epoch 1 Batch 7641 Loss 0.6931480765342712 Running Loss 0.6929578696335038\n",
            "Epoch 1 Batch 7651 Loss 0.693147599697113 Running Loss 0.69295811773889\n",
            "Epoch 1 Batch 7661 Loss 0.6931471824645996 Running Loss 0.6929583650254002\n",
            "Epoch 1 Batch 7671 Loss 0.6931483745574951 Running Loss 0.6929586114573861\n",
            "Epoch 1 Batch 7681 Loss 0.6931471824645996 Running Loss 0.6929588576357062\n",
            "Epoch 1 Batch 7691 Loss 0.6931475400924683 Running Loss 0.6929591043208421\n",
            "Epoch 1 Batch 7701 Loss 0.6931473016738892 Running Loss 0.6929593489876259\n",
            "Epoch 1 Batch 7711 Loss 0.6931474804878235 Running Loss 0.6929595931666848\n",
            "Epoch 1 Batch 7721 Loss 0.693148136138916 Running Loss 0.6929598372999428\n",
            "Epoch 1 Batch 7731 Loss 0.6931475400924683 Running Loss 0.6929600808864391\n",
            "Epoch 1 Batch 7741 Loss 0.6931474804878235 Running Loss 0.692960323073608\n",
            "Epoch 1 Batch 7751 Loss 0.6931472420692444 Running Loss 0.6929605650280448\n",
            "Epoch 1 Batch 7761 Loss 0.6931471824645996 Running Loss 0.6929608059519269\n",
            "Epoch 1 Batch 7771 Loss 0.6931485533714294 Running Loss 0.692961047306559\n",
            "Epoch 1 Batch 7781 Loss 0.6931471824645996 Running Loss 0.6929612890596394\n",
            "Epoch 1 Batch 7791 Loss 0.6931471824645996 Running Loss 0.692961528164755\n",
            "Epoch 1 Batch 7801 Loss 0.6931471824645996 Running Loss 0.6929617662748272\n",
            "Epoch 1 Batch 7811 Loss 0.6931484341621399 Running Loss 0.69296200412624\n",
            "Epoch 1 Batch 7821 Loss 0.6931471824645996 Running Loss 0.6929622428707722\n",
            "Epoch 1 Batch 7831 Loss 0.6931473612785339 Running Loss 0.6929624792853926\n",
            "Epoch 1 Batch 7841 Loss 0.693148136138916 Running Loss 0.6929627152566261\n",
            "Epoch 1 Batch 7851 Loss 0.6931471824645996 Running Loss 0.6929629511050306\n",
            "Epoch 1 Batch 7861 Loss 0.6931473016738892 Running Loss 0.6929631858908665\n",
            "Epoch 1 Batch 7871 Loss 0.693147599697113 Running Loss 0.6929634199210913\n",
            "Epoch 1 Batch 7881 Loss 0.6931477189064026 Running Loss 0.6929636533498431\n",
            "Epoch 1 Batch 7891 Loss 0.6931471824645996 Running Loss 0.6929638865646369\n",
            "Epoch 1 Batch 7901 Loss 0.6931474804878235 Running Loss 0.6929641188194353\n",
            "Epoch 1 Batch 7911 Loss 0.693147599697113 Running Loss 0.6929643511802294\n",
            "Epoch 1 Batch 7921 Loss 0.6931475400924683 Running Loss 0.6929645824125358\n",
            "Epoch 1 Batch 7931 Loss 0.6931473612785339 Running Loss 0.6929648130016088\n",
            "Epoch 1 Batch 7941 Loss 0.6931472420692444 Running Loss 0.6929650429198551\n",
            "Epoch 1 Batch 7951 Loss 0.693149209022522 Running Loss 0.6929652728145039\n",
            "Epoch 1 Batch 7961 Loss 0.6931474208831787 Running Loss 0.6929655018845271\n",
            "Epoch 1 Batch 7971 Loss 0.6931473016738892 Running Loss 0.6929657303723139\n",
            "Epoch 1 Batch 7981 Loss 0.6931471824645996 Running Loss 0.6929659577722075\n",
            "Epoch 1 Batch 7991 Loss 0.6931473016738892 Running Loss 0.6929661850877943\n",
            "Epoch 1 Batch 8001 Loss 0.6931472420692444 Running Loss 0.6929664113807344\n",
            "Epoch 1 Batch 8011 Loss 0.6931472420692444 Running Loss 0.6929666371533612\n",
            "Epoch 1 Batch 8021 Loss 0.6931477785110474 Running Loss 0.6929668628980715\n",
            "Epoch 1 Batch 8031 Loss 0.6931473612785339 Running Loss 0.6929670877391947\n",
            "Epoch 1 Batch 8041 Loss 0.6931475400924683 Running Loss 0.6929673119469553\n",
            "Epoch 1 Batch 8051 Loss 0.6931477785110474 Running Loss 0.692967536271455\n",
            "Epoch 1 Batch 8061 Loss 0.6931471824645996 Running Loss 0.692967759617918\n",
            "Epoch 1 Batch 8071 Loss 0.6931472420692444 Running Loss 0.6929679825733975\n",
            "Epoch 1 Batch 8081 Loss 0.6931473016738892 Running Loss 0.6929682047631743\n",
            "Epoch 1 Batch 8091 Loss 0.6931471824645996 Running Loss 0.6929684260574853\n",
            "Epoch 1 Batch 8101 Loss 0.6931473016738892 Running Loss 0.6929686472395616\n",
            "Epoch 1 Batch 8111 Loss 0.6931471824645996 Running Loss 0.6929688677880668\n",
            "Epoch 1 Batch 8121 Loss 0.6931477785110474 Running Loss 0.692969087756718\n",
            "Epoch 1 Batch 8131 Loss 0.6931471824645996 Running Loss 0.6929693075361734\n",
            "Epoch 1 Batch 8141 Loss 0.6931471824645996 Running Loss 0.6929695265926581\n",
            "Epoch 1 Batch 8151 Loss 0.6931472420692444 Running Loss 0.6929697447972069\n",
            "Epoch 1 Batch 8161 Loss 0.6931479573249817 Running Loss 0.6929699628467931\n",
            "Epoch 1 Batch 8171 Loss 0.6931477189064026 Running Loss 0.6929701808368163\n",
            "Epoch 1 Batch 8181 Loss 0.6931471824645996 Running Loss 0.6929703977183482\n",
            "Epoch 1 Batch 8191 Loss 0.693147599697113 Running Loss 0.6929706137283077\n",
            "Epoch 1 Batch 8201 Loss 0.6931471824645996 Running Loss 0.6929708294222492\n",
            "Epoch 1 Batch 8211 Loss 0.6931473612785339 Running Loss 0.6929710450989511\n",
            "Epoch 1 Batch 8221 Loss 0.6931471824645996 Running Loss 0.692971259678183\n",
            "Epoch 1 Batch 8231 Loss 0.6931472420692444 Running Loss 0.6929714738446443\n",
            "Epoch 1 Batch 8241 Loss 0.6931472420692444 Running Loss 0.6929716873611587\n",
            "Epoch 1 Batch 8251 Loss 0.6931493282318115 Running Loss 0.6929719005696139\n",
            "Epoch 1 Batch 8261 Loss 0.6931473016738892 Running Loss 0.6929721129011291\n",
            "Epoch 1 Batch 8271 Loss 0.6931471824645996 Running Loss 0.6929723251155636\n",
            "Epoch 1 Batch 8281 Loss 0.6931471824645996 Running Loss 0.6929725366231252\n",
            "Epoch 1 Batch 8291 Loss 0.693148136138916 Running Loss 0.6929727480590107\n",
            "Epoch 1 Batch 8301 Loss 0.6931471824645996 Running Loss 0.6929729583464161\n",
            "Epoch 1 Batch 8311 Loss 0.6931474208831787 Running Loss 0.6929731681994935\n",
            "Epoch 1 Batch 8321 Loss 0.6931472420692444 Running Loss 0.6929733780639245\n",
            "Epoch 1 Batch 8331 Loss 0.6931477785110474 Running Loss 0.6929735875390126\n",
            "Epoch 1 Batch 8341 Loss 0.6931471824645996 Running Loss 0.6929737959044141\n",
            "Epoch 1 Batch 8351 Loss 0.6931472420692444 Running Loss 0.6929740036851474\n",
            "Epoch 1 Batch 8361 Loss 0.6931472420692444 Running Loss 0.6929742110544037\n",
            "Epoch 1 Batch 8371 Loss 0.6931471824645996 Running Loss 0.6929744178000466\n",
            "Epoch 1 Batch 8381 Loss 0.6931473016738892 Running Loss 0.6929746240594339\n",
            "Epoch 1 Batch 8391 Loss 0.6931471824645996 Running Loss 0.6929748297916836\n",
            "Epoch 1 Batch 8401 Loss 0.6931471824645996 Running Loss 0.6929750352470013\n",
            "Epoch 1 Batch 8411 Loss 0.6931488513946533 Running Loss 0.6929752401641739\n",
            "Epoch 1 Batch 8421 Loss 0.6931473612785339 Running Loss 0.6929754446088213\n",
            "Epoch 1 Batch 8431 Loss 0.6931472420692444 Running Loss 0.6929756488088554\n",
            "Epoch 1 Batch 8441 Loss 0.693147599697113 Running Loss 0.6929758522567302\n",
            "Epoch 1 Batch 8451 Loss 0.6931473016738892 Running Loss 0.6929760551667047\n",
            "Epoch 1 Batch 8461 Loss 0.6931472420692444 Running Loss 0.6929762575125077\n",
            "Epoch 1 Batch 8471 Loss 0.6931471824645996 Running Loss 0.6929764598590427\n",
            "Epoch 1 Batch 8481 Loss 0.6931485533714294 Running Loss 0.6929766615105327\n",
            "Epoch 1 Batch 8491 Loss 0.6931471824645996 Running Loss 0.6929768626238682\n",
            "Epoch 1 Batch 8501 Loss 0.6931475400924683 Running Loss 0.6929770638389933\n",
            "Epoch 1 Batch 8511 Loss 0.6931474208831787 Running Loss 0.6929772640420329\n",
            "Epoch 1 Batch 8521 Loss 0.6931477785110474 Running Loss 0.6929774646285609\n",
            "Epoch 1 Batch 8531 Loss 0.6931471824645996 Running Loss 0.692977663997245\n",
            "Epoch 1 Batch 8541 Loss 0.6931473016738892 Running Loss 0.6929778627595051\n",
            "Epoch 1 Batch 8551 Loss 0.6931471824645996 Running Loss 0.6929780609383804\n",
            "Epoch 1 Batch 8561 Loss 0.6931474208831787 Running Loss 0.6929782590441664\n",
            "Epoch 1 Batch 8571 Loss 0.6931488513946533 Running Loss 0.6929784566250945\n",
            "Epoch 1 Batch 8581 Loss 0.6931475400924683 Running Loss 0.6929786537594068\n",
            "Epoch 1 Batch 8591 Loss 0.6931471824645996 Running Loss 0.6929788506706801\n",
            "Epoch 1 Batch 8601 Loss 0.6931483745574951 Running Loss 0.6929790468468747\n",
            "Epoch 1 Batch 8611 Loss 0.6931471824645996 Running Loss 0.6929792423113175\n",
            "Epoch 1 Batch 8621 Loss 0.6931471824645996 Running Loss 0.6929794374674908\n",
            "Epoch 1 Batch 8631 Loss 0.6931483149528503 Running Loss 0.69297963266176\n",
            "Epoch 1 Batch 8641 Loss 0.6931472420692444 Running Loss 0.6929798266592712\n",
            "Epoch 1 Batch 8651 Loss 0.6931473612785339 Running Loss 0.6929800207525881\n",
            "Epoch 1 Batch 8661 Loss 0.6931471824645996 Running Loss 0.6929802142600652\n",
            "Epoch 1 Batch 8671 Loss 0.6931473016738892 Running Loss 0.6929804069362645\n",
            "Epoch 1 Batch 8681 Loss 0.693147599697113 Running Loss 0.6929805992028911\n",
            "Epoch 1 Batch 8691 Loss 0.6931474208831787 Running Loss 0.692980791575724\n",
            "Epoch 1 Batch 8701 Loss 0.6931471824645996 Running Loss 0.6929809831090528\n",
            "Epoch 1 Batch 8711 Loss 0.6931473612785339 Running Loss 0.6929811740657819\n",
            "Epoch 1 Batch 8721 Loss 0.6931476593017578 Running Loss 0.6929813646119254\n",
            "Epoch 1 Batch 8731 Loss 0.6931477189064026 Running Loss 0.6929815550219657\n",
            "Epoch 1 Batch 8741 Loss 0.6931474804878235 Running Loss 0.6929817450713435\n",
            "Epoch 1 Batch 8751 Loss 0.6931473016738892 Running Loss 0.6929819342572681\n",
            "Epoch 1 Batch 8761 Loss 0.6931474208831787 Running Loss 0.6929821230657377\n",
            "Epoch 1 Batch 8771 Loss 0.6931471824645996 Running Loss 0.6929823114776568\n",
            "Epoch 1 Batch 8781 Loss 0.6931473016738892 Running Loss 0.6929824993857734\n",
            "Epoch 1 Batch 8791 Loss 0.6931472420692444 Running Loss 0.6929826867850265\n",
            "Epoch 1 Batch 8801 Loss 0.6931473612785339 Running Loss 0.6929828737516482\n",
            "Epoch 1 Batch 8811 Loss 0.6931471824645996 Running Loss 0.692983060435937\n",
            "Epoch 1 Batch 8821 Loss 0.6931471824645996 Running Loss 0.6929832465753252\n",
            "Epoch 1 Batch 8831 Loss 0.6931472420692444 Running Loss 0.692983432502388\n",
            "Epoch 1 Batch 8841 Loss 0.693149209022522 Running Loss 0.6929836181504277\n",
            "Epoch 1 Batch 8851 Loss 0.6931472420692444 Running Loss 0.6929838033183633\n",
            "Epoch 1 Batch 8861 Loss 0.693148136138916 Running Loss 0.6929839882028922\n",
            "Epoch 1 Batch 8871 Loss 0.6931473612785339 Running Loss 0.692984172509335\n",
            "Epoch 1 Batch 8881 Loss 0.6931476593017578 Running Loss 0.6929843562866248\n",
            "Epoch 1 Batch 8891 Loss 0.6931471824645996 Running Loss 0.6929845394963234\n",
            "Epoch 1 Batch 8901 Loss 0.6931472420692444 Running Loss 0.6929847223613251\n",
            "Epoch 1 Batch 8911 Loss 0.693147599697113 Running Loss 0.6929849050700791\n",
            "Epoch 1 Batch 8921 Loss 0.6931473612785339 Running Loss 0.6929850872823602\n",
            "Epoch 1 Batch 8931 Loss 0.6931472420692444 Running Loss 0.6929852688596839\n",
            "Epoch 1 Batch 8941 Loss 0.6931472420692444 Running Loss 0.6929854502908309\n",
            "Epoch 1 Batch 8951 Loss 0.6931477785110474 Running Loss 0.6929856315429964\n",
            "Epoch 1 Batch 8961 Loss 0.6931471824645996 Running Loss 0.6929858119715778\n",
            "Epoch 1 Batch 8971 Loss 0.6931471824645996 Running Loss 0.6929859921507261\n",
            "Epoch 1 Batch 8981 Loss 0.6931471824645996 Running Loss 0.6929861719817232\n",
            "Epoch 1 Batch 8991 Loss 0.6931472420692444 Running Loss 0.6929863511541504\n",
            "Epoch 1 Batch 9001 Loss 0.693147599697113 Running Loss 0.6929865299417051\n",
            "Epoch 1 Batch 9011 Loss 0.6931471824645996 Running Loss 0.6929867082530631\n",
            "Epoch 1 Batch 9021 Loss 0.6931471824645996 Running Loss 0.6929868864069597\n",
            "Epoch 1 Batch 9031 Loss 0.6931471824645996 Running Loss 0.6929870642587178\n",
            "Epoch 1 Batch 9041 Loss 0.6931488513946533 Running Loss 0.6929872421719386\n",
            "Epoch 1 Batch 9051 Loss 0.6931472420692444 Running Loss 0.6929874190993367\n",
            "Epoch 1 Batch 9061 Loss 0.6931472420692444 Running Loss 0.6929875957085694\n",
            "Epoch 1 Batch 9071 Loss 0.6931471824645996 Running Loss 0.6929877725789282\n",
            "Epoch 1 Batch 9081 Loss 0.6931475400924683 Running Loss 0.6929879482524167\n",
            "Epoch 1 Batch 9091 Loss 0.6931474208831787 Running Loss 0.6929881238475805\n",
            "Epoch 1 Batch 9101 Loss 0.6931473612785339 Running Loss 0.6929882988603858\n",
            "Epoch 1 Batch 9111 Loss 0.6931473612785339 Running Loss 0.6929884737441522\n",
            "Epoch 1 Batch 9121 Loss 0.6931473016738892 Running Loss 0.6929886482575133\n",
            "Epoch 1 Batch 9131 Loss 0.6931471824645996 Running Loss 0.6929888221862712\n",
            "Epoch 1 Batch 9141 Loss 0.6931477189064026 Running Loss 0.6929889955975506\n",
            "Epoch 1 Batch 9151 Loss 0.6931471824645996 Running Loss 0.6929891687340456\n",
            "Epoch 1 Batch 9161 Loss 0.6931471824645996 Running Loss 0.6929893414470101\n",
            "Epoch 1 Batch 9171 Loss 0.6931473016738892 Running Loss 0.6929895140432945\n",
            "Epoch 1 Batch 9181 Loss 0.6931473016738892 Running Loss 0.6929896863479913\n",
            "Epoch 1 Batch 9191 Loss 0.6931478381156921 Running Loss 0.6929898580702225\n",
            "Epoch 1 Batch 9201 Loss 0.6931474208831787 Running Loss 0.6929900300864253\n",
            "Epoch 1 Batch 9211 Loss 0.6931493282318115 Running Loss 0.6929902010108423\n",
            "Epoch 1 Batch 9221 Loss 0.6931477785110474 Running Loss 0.6929903716679549\n",
            "Epoch 1 Batch 9231 Loss 0.6931473016738892 Running Loss 0.6929905422265139\n",
            "Epoch 1 Batch 9241 Loss 0.6931478977203369 Running Loss 0.6929907122804881\n",
            "Epoch 1 Batch 9251 Loss 0.6931471824645996 Running Loss 0.6929908819023873\n",
            "Epoch 1 Batch 9261 Loss 0.6931475400924683 Running Loss 0.6929910510936111\n",
            "Epoch 1 Batch 9271 Loss 0.6931471824645996 Running Loss 0.6929912197398286\n",
            "Epoch 1 Batch 9281 Loss 0.6931471824645996 Running Loss 0.692991388266668\n",
            "Epoch 1 Batch 9291 Loss 0.693147599697113 Running Loss 0.6929915560778909\n",
            "Epoch 1 Batch 9301 Loss 0.6931474804878235 Running Loss 0.6929917238422805\n",
            "Epoch 1 Batch 9311 Loss 0.6931472420692444 Running Loss 0.6929918910734714\n",
            "Epoch 1 Batch 9321 Loss 0.6931475400924683 Running Loss 0.6929920580737288\n",
            "Epoch 1 Batch 9331 Loss 0.693147599697113 Running Loss 0.6929922246585487\n",
            "Epoch 1 Batch 9341 Loss 0.6931471824645996 Running Loss 0.6929923908803132\n",
            "Epoch 1 Batch 9351 Loss 0.6931476593017578 Running Loss 0.6929925566573231\n",
            "Epoch 1 Batch 9361 Loss 0.6931473612785339 Running Loss 0.6929927219527996\n",
            "Epoch 1 Batch 9371 Loss 0.6931476593017578 Running Loss 0.6929928870099851\n",
            "Epoch 1 Batch 9381 Loss 0.6931471824645996 Running Loss 0.6929930536277563\n",
            "Epoch 1 Batch 9391 Loss 0.6931473016738892 Running Loss 0.6929932184308726\n",
            "Epoch 1 Batch 9401 Loss 0.6931476593017578 Running Loss 0.692993382566369\n",
            "Epoch 1 Batch 9411 Loss 0.6931472420692444 Running Loss 0.692993546587389\n",
            "Epoch 1 Batch 9421 Loss 0.6931471824645996 Running Loss 0.6929937103424542\n",
            "Epoch 1 Batch 9431 Loss 0.6931485533714294 Running Loss 0.6929938736680886\n",
            "Epoch 1 Batch 9441 Loss 0.6931478381156921 Running Loss 0.6929940362499877\n",
            "Epoch 1 Batch 9451 Loss 0.6931476593017578 Running Loss 0.6929941987779428\n",
            "Epoch 1 Batch 9461 Loss 0.6931511163711548 Running Loss 0.692994361264725\n",
            "Epoch 1 Batch 9471 Loss 0.6931471824645996 Running Loss 0.6929945227916308\n",
            "Epoch 1 Batch 9481 Loss 0.6931474804878235 Running Loss 0.6929946841035334\n",
            "Epoch 1 Batch 9491 Loss 0.6931473612785339 Running Loss 0.6929948449687477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7bcb8b199d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                             \u001b[0mtoken_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0mvirus_domain_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimmigrant_domain_regex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1510\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m         )\n\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m         )\n\u001b[1;32m    971\u001b[0m         encoder_outputs = self.encoder(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (578) must match the size of tensor b (512) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "JCnow78Btq9R",
        "outputId": "cd3f8c45-4e8d-4ab3-becf-9f538933765f"
      },
      "source": [
        "# now look for when the model is ambiguous about how to classify a sentence\n",
        "i = 1\n",
        "samples = []\n",
        "for j, s in enumerate(dataset['train']):\n",
        "    if j < 1_511:\n",
        "        continue\n",
        "    for paragraph in s['text'].split('\\n'):\n",
        "        sentences = re.findall(r'.*?[.?!]', paragraph)\n",
        "        for sentence in sentences:\n",
        "            if sentence.strip() and (virus_domain_regex.search(sentence) or immigrant_domain_regex.search(sentence)):\n",
        "                with torch.no_grad():\n",
        "                    try:\n",
        "                        target = bert(**bert_tokenizer(sentence, return_tensors='pt').to(device))\n",
        "                    except:\n",
        "                        continue\n",
        "                    sigmoid = torch.sigmoid(target.logits).item()\n",
        "                if sigmoid <= 0.3:\n",
        "                    samples.append((sentence, 0, sigmoid))\n",
        "                elif sigmoid >= 0.7:\n",
        "                    samples.append((sentence, 1, sigmoid))\n",
        "                else:\n",
        "                    samples.append((sentence, 2, sigmoid))\n",
        "                i += 1\n",
        "                if i % 500 == 0:\n",
        "                    print('On sample', i)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6623da722bdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1_511\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "rgbVy2bHvfEb",
        "outputId": "46029742-f6d0-4462-e3ad-3eeaef17a0bf"
      },
      "source": [
        "df = pd.DataFrame(data=samples, columns=['text', 'target', 'sigmoid'])\n",
        "df[df['target'] == 2].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "      <th>sigmoid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>157.0</td>\n",
              "      <td>157.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.532379</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.100923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.310892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.447861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.523684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.629069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.699679</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       target     sigmoid\n",
              "count   157.0  157.000000\n",
              "mean      2.0    0.532379\n",
              "std       0.0    0.100923\n",
              "min       2.0    0.310892\n",
              "25%       2.0    0.447861\n",
              "50%       2.0    0.523684\n",
              "75%       2.0    0.629069\n",
              "max       2.0    0.699679"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "iKoj-8DjZtg9",
        "outputId": "8259e7a2-6329-4d29-e1b5-857823577f65"
      },
      "source": [
        "df[df['target'] == 2].iloc[3].text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Trump also related the opioid crisis to immigration, saying he will work to end sanctuary city policies and accusing Democrats of stonewalling progress on DACA because they want to stop construction of the border wall.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRsurUSxvoe4"
      },
      "source": [
        "# you can download from the directory (we can concatenate all of our efforts together)\n",
        "df[['target', 'sigmoid', 'text']].sort_values(by=['target', 'sigmoid', 'text'], ascending=[False, False, True]).to_csv('immigration.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3woPdzvv3V8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}